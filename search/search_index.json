{"docs":[{"location":"/index.html","text":"Warning This site is still a work in progress #1. Until migration is complete, refer to: Atlas Documentation Spectator Documentation","title":"Atlas"},{"location":"/index.html#atlas","text":"Atlas was developed by Netflix to manage dimensional time series data for near real-time operational insight. Atlas features in-memory data storage, allowing it to gather and report very large numbers of metrics, very quickly.\nAtlas captures operational intelligence. Whereas business intelligence is data gathered for analyzing trends over time, operational intelligence provides a picture of what is currently happening within a system.\nAtlas was built because the existing systems Netflix was using for operational intelligence were not able to cope with the increase in metrics we were seeing as we expanded our operations in the cloud. In 2011, we were monitoring 2 million metrics related to our streaming systems. By 2014, we were at 1.2 billion metrics and the numbers continue to rise. Atlas is designed to handle this large quantity of data and can scale with the hardware we use to analyze and store it.\nFor details and background on the project please read through the overview page.\nCheck out the getting started page for an introduction to using Atlas in the cloud environment. Once you’ve explored the example, check out the stack language references to see the various types of information you can access.","title":"Atlas"},{"location":"/overview.html","text":"","title":"Overview"},{"location":"/overview.html#overview","text":"Atlas is the system Netflix uses to manage dimensional time-series data for near real-time operational insight. It was primarily created to address issues with scale and query capability in the previous system.","title":"Overview"},{"location":"/overview.html#history","text":"In May of 2011, Netflix was using a home-grown solution called Epic to manage time-series data. Epic was a combination of perl CGI scripts, RRDTool logging, and MySQL. We were tracking around 2M distinct time series and the monitoring system was regularly failing to keep up with the volume of data. In addition there were a number of trends in the company which presaged a drastic increase in metric volume:\nRolling pushes to Red/Black deployments. Leveraging auto-scaling for large clusters. Netflix has always used auto-scaling groups in AWS, but initially most were configured with fixed size and just used as a group and to replace instances. Expansion internationally into Latin America and Europe. This led to an increase in the number of countries being tracked for key metrics and for Europe it was the first move into additional AWS regions. With additional regions we also wanted to have better isolation so a problem with monitoring in one region would not impact another, but at the same time have a mechanism to provide a global view if needed.\nSince that time the metric volume has continued to grow quickly. The graph below shows the increase in metrics measured over last few years:\nThe growth in raw volume required increased query capability to actually use the data.","title":"History"},{"location":"/overview.html#goals","text":"The main goals for Atlas were to build a system that provided:\nA Common API Scale Dimensionality","title":"Goals"},{"location":"/overview.html#common-api","text":"Epic did a number of things really well that we didn’t want to lose when transitioning. In particular:\nNormalization and consolidation Flexible legends that scale independently of the chart Math, especially handling of NaN values representing no data Holt-Winters used for alerting Visualization options Deep linking\nMany of these are capabilities that are provided by the RRDTool library Epic was using, but most alternatives we looked at fell short in these categories. In addition, we have uses for other 3rd party services like CloudWatch and it is desirable to have common query capability for that data.","title":"Common API"},{"location":"/overview.html#scale","text":"As indicated in the history section, metrics volume was growing and we needed a system that could keep up. For a long time our biggest concern was write volume, however, we also wanted to scale in terms of the amount of data we could read or aggregate as part of a graph request.","title":"Scale"},{"location":"/overview.html#dimensionality","text":"This is a decision that was made because users were already doing it in ways that were hard to support. Epic only support a simple name with some special case system dimensions of cluster and node. Many users were creating names like:\ncom.netflix.eds.nccp.successful.requests.uiversion.nccprt-authorization.devtypid-101.clver-PHL_0AB.uiver-UI_169_mid.geo-US\nThat breaks down to:\nKey Value name com.netflix.eds.nccp.successful.requests.uiversion nccprt authorization devtypid 101 clver PHL_0AB uiver UI_169_mid geo US\nSince it was all mangled into a name with different conventions by team, users would have to resort to complex regular expressions to slice and dice the data based on the dimensions.","title":"Dimensionality"},{"location":"/overview.html#query-layer","text":"In order to get a common API, have flexibility for backend implementations, and provide merged views across backends we built a query layer that can be hierarchically composed. The diagram below shows the main Netflix setup:\nWe have isolated regional deployments in each region we operate in as well as a global deployment that can combine the results from multiple regions. The query and aggregation operations can be performed on the fan out so most of the big summarization operations will distribute the computation across the tree and typically to an optimized storage layer at some point.\nAllowing the query and rendering layer to work on multiple backends also makes it easier for us to consider transitioning to other backends in the future such as OpenTSDB or InfluxDB. Switching to Atlas one of the biggest hurdles was compatibility and transitioning to the new system.","title":"Query Layer"},{"location":"/overview.html#stack-language","text":"One of our key requirements was to be able to have deep links into a particular chart and to be able to reliably pass around or embed these images via email, wikis, html pages, etc. In addition, the user who receives the link should be able to tweak the result. Atlas uses a simple stack language that has a minimal punctuation and allows arbitrarily complex graph expressions to be encoded in a URL friendly way. This means that all images can be accessed using a GET request. The stack language is also simple to parse and interpret, allowing it to be easily consumed from a variety of tools. The core features include:\nEmbedding and linking using a GET request URL friendly stack language Few special symbols (comma, colon, parenthesis) Easy to extend Basic operations Query: and, or, equal, regex, has key, not Aggregation: sum, count, min, max, group by Consolidation: aggregate across time Math: add, subtract, multiply, etc Boolean: and, or, lt, gt, etc Graph settings: legends, area, transparency","title":"Stack Language"},{"location":"/overview.html#graph-example","text":"To illustrate, this is a sample graph image:\nThis graph shows the number of requests per second and compares that with a prediction line generated using double exponential smoothing. If the number of requests per second falls below the prediction, it indicates an alert would trigger using the vertical spans. The url to generate this image follows (newlines added for readability):\nhttp://atlas/api/v1/graph\n  ?tz=UTC\n  &e=2012-01-01T08:00\n  &s=e-8h\n  &w=500\n  &h=150\n  &l=0\n  &q=nf.cluster,alerttest,:eq,\n     name,requestsPerSecond,:eq,:and,\n     :sum,\n     :dup,10,0.1,0.02,:des,\n     0.85,:mul,\n     :2over,:lt,\n     :rot,$name,:legend,\n     :rot,prediction,:legend,\n     :rot,:vspan,60,:alpha,alert+triggered,:legend\nAdding some comments to the stack expression to explain a bit what is going on:\n# Query to generate the input line\nnf.cluster,alerttest,:eq,\nname,requestsPerSecond,:eq,:and,\n:sum,\n\n# Create a copy on the stack\n:dup,\n\n# Apply a DES function to generate a prediction\n# using the copy on the top of the stack. For\n# a description of the parameters see the DES\n# reference page.\n10,0.1,0.02,:des,\n\n# Used to set a threshold. The prediction should\n# be roughly equal to the line, in this case the\n# threshold would be 85% of the prediction.\n0.85,:mul,\n\n# Before              After\n# 4.                  4. actual\n# 3.                  3. prediction\n# 2. actual           2. actual\n# 1. prediction       1. prediction\n:2over,\n\n# Create a boolean signal line that is 1\n# for datapoints where the actual value is\n# less than the prediction and 0 where it\n# is greater than or equal the prediction.\n# The 1 values are where the alert should\n# trigger.\n:lt,\n\n# Apply presentation details.\n:rot,$name,:legend,\n:rot,prediction,:legend,\n:rot,:vspan,60,:alpha,alert+triggered,:legend\nSee the [stack language](Stack Language) page for more information.","title":"Graph Example"},{"location":"/overview.html#memory-storage","text":"Storage for Atlas has been a bit of a sore point. We have tried many backends and ended up moving more and more to a model where pretty much all data is stored in memory either in or off the java heap.","title":"Memory Storage"},{"location":"/overview.html#speed","text":"The primary goal for Atlas is to support queries over dimensional time series data so we can slice and dice to drill down into problems. This means we frequently have a need to perform a large aggregations that involve many data points even though the final result set might be small.\nAs an example consider a simple graph showing the number of requests per second hitting a service for the last 3 hours. Assuming minute resolution that is 180 datapoints for the final output. On a typical service we would get one time series per node showing the number of requests so if we have 100 nodes the intermediate result set is around 18k datapoints. For one service users went hog wild with dimensions breaking down requests by device (~1000s) and country (~50) leading to about 50k time series per node. If we still assume 100 nodes that is about 900M datapoints for the same 3h line.\nThough obviously we have to be mindful about the explosion of dimensions, we also want that where possible to be a decision based on cost and business value rather than a technical limitation.","title":"Speed"},{"location":"/overview.html#resilience","text":"What all has to be working in order for the monitoring system to work? If it falls over what is involved in getting it back up? Our focus is primarily operational insight so the top priority is to be able to determine what is going on right now. This leads to the following rules of thumb:\nData becomes exponentially less important as it gets older Restoring service is more important than preventing data loss Try to degrade gracefully\nAs a result the internal Atlas deployment breaks up the data into multiple windows based on the window of data they contain.\nWith this setup we can show the last 6h of data as long as clients can successfully publish. The data is all in memory sharded across machines in the 6h clusters. Because the data and index are all in memory on the local node each instance is self contained and doesn’t need any external service to function. We typically run multiple mirrors of the 6h cluster so data is replicated and we can handle loss of an instance. In AWS we run each mirror in a different zone so that a zone failure will only impact a single mirror.\nThe publish cluster needs to know all of the instance in the mirror cluster and takes care of splitting the traffic up so it goes to the correct shard. The set of mirror instances and shards are assigned based on slots from the Edda autoScalingGroups API. Since the set of instances for the mirrors change rarely, the publish instances can cache the Edda response and still retain successfully publish most data if Edda fails. If an instance is replaced and we can’t update data we would have partial loss for a single shard if the same shard was missing in another mirror.\nHistorical data can also fail in which case graphs would not be able to show data for some older windows. This doesn’t have to be fully continuous, for example a common use case for us is to look at week-over-week (WoW) charts even though the span of the chart might only be a few hours. If the < 4d cluster fails but the < 16d cluster is functioning we could still serve that graph even though we couldn’t show a continuous graph for the full week. A graph would still be shown but would be missing data in the middle.\nAfter data is written to the mirrors, they will flush to a persistence layer that is responsible for writing the data to the long term storage in S3. The data at full resolution is kept in S3 and we use hadoop (Elastic MapReduce) for processing the data to perform corrective merging of data from the mirrors, generate reports, and perform rollups into a form that can be loaded into the historical clusters.","title":"Resilience"},{"location":"/overview.html#cost","text":"Keeping all data in memory is expensive in-particular with the large growth rate of data. The combination of dimensionality and time based partitioning used for resilience also give us a way to help manage costs.\nThe first way is in controlling the number of replicas. In most cases we are using replicas for redundancy not to provide additional query capacity. For historical data that can be reloaded from stable storage we typically run only one replica as the duration of partial downtime was not deemed to be worth the cost for an additional replica.\nThe second way is as part of the hadoop processing we can compute rollups so that we have a much smaller data volume to load in historical clusters. At Netflix the typical policy is roughly:\nCluster Policy < 6h Keeps all data received < 4d ago Keeps most data, we do early rollup by dropping the node dimension on some business metrics < 16d ago Rollup by dropping the node dimension on all metrics older Explicit whitelist, typically recommend BI systems for these use-cases\nUsing these policies we get greatly reduced index sizes for the number of distinct time series despite a significant amount of churn. With auto-scaling and red/black deployment models the set of instances change frequently so typically the intersection of distinct time series from one day to the next is less than 50%. Rollups target the dimensions which lead to that churn giving us much smaller index sizes. Also, in many cases dimensions like node that lead to this increase become less relevant after the node goes away. Deep-dive or investigative use-cases can still access the data using hadoop if needed.\nSnapshot of index sizes for one region in our environment:\n< 6h < 4d < 16d","title":"Cost"},{"location":"/overview.html#ecosystem","text":"Internally there is a lot of tooling and infrastructure built up around Atlas. We are planning to open source many of these tools as time permits. This project is the first step for that with the query layer and some of the in-heap memory storage. Some additional parts that should come in the future:\nUser interfaces Main UI for browsing data and constructing queries. Dashboards Alerts Platform Inline aggregation of reported data before storage layer Storage options using off-heap memory and lucene Percentile backend Publish and persistence applications EMR processing for computing rollups and analysis Poller for SNMP, healthchecks, etc Client Supports integrating servo with Atlas Local rollups and alerting Analytics Metrics volume report Canary analysis Outlier and anomaly detection\nThese projects were originally developed and run internally and thus only needed to be setup by our team and assume many internal infrastructure pieces to run. There is a goal to try and make this easier, but it will take some time.","title":"Ecosystem"},{"location":"/getting-started.html","text":"","title":"Getting Started"},{"location":"/getting-started.html#getting-started","text":"The instructions on this page are for quickly getting a sample backend server running on a local machine. For other common tasks see:\nQuerying Data: Examples Tutorial Instrumenting Code","title":"Getting Started"},{"location":"/getting-started.html#run-a-demo-instance","text":"Prerequisites\nThese instructions assume a unix based machine with curl. Other systems may work, but have not been tried. Java 8 or higher is required.\nTo quickly run a version with some synthetic sample data:\n$ curl -LO https://github.com/Netflix/atlas/releases/download/v1.5.3/atlas-1.5.3-standalone.jar\n$ java -jar atlas-1.5.3-standalone.jar","title":"Run a Demo Instance"},{"location":"/getting-started.html#explore-available-tags","text":"The tags API is used to explore available tags and the relationships between them.\n# show all tags\n$ curl -s 'http://localhost:7101/api/v1/tags'\n\n# show all values of the name, nf.app and type tags\n$ curl -s 'http://localhost:7101/api/v1/tags/name'\n$ curl -s 'http://localhost:7101/api/v1/tags/nf.app'\n$ curl -s 'http://localhost:7101/api/v1/tags/type'\n\n# show all name tags that also have the type tag\n$ curl -s 'http://localhost:7101/api/v1/tags/name?q=type,:has'\n\n# show all name tags that have an nf.app tag with a value of nccp\n$ curl -s 'http://localhost:7101/api/v1/tags/name?q=nf.app,nccp,:eq'","title":"Explore Available Tags"},{"location":"/getting-started.html#generate-graphs","text":"These graph API URLs show off a couple of the capabilities of the Atlas backend. See the Examples page for more detailed use cases.\n# graph all metrics with a name tag value of ssCpuUser, using an :avg aggregation\n$ curl -Lo graph.png 'http://localhost:7101/api/v1/graph?q=name,ssCpuUser,:eq,:avg'\n\n# duplicate the ssCpuUser signal, check if it is greater than 22.8 and display the result as a vertical span with 30% alpha\n$ curl -Lo graph.png 'http://localhost:7101/api/v1/graph?q=name,ssCpuUser,:eq,:avg,:dup,22.8,:gt,:vspan,30,:alpha'","title":"Generate Graphs"},{"location":"/getting-started.html#running-demo-with-memory-storage","text":"Run an instance with a configuration to use the memory storage:\n$ curl -Lo memory.conf https://raw.githubusercontent.com/Netflix/atlas/v1.5.x/conf/memory.conf\n$ java -jar atlas-1.5.3-standalone.jar memory.conf\nNow we can send some data to it. To quickly get started there is a sample script to send in some data:\n$ curl -Lo publish-test.sh https://raw.githubusercontent.com/Netflix/atlas/v1.5.x/scripts/publish-test.sh\n$ chmod 755 publish-test.sh\n$ ./publish-test.sh\nThen view the data in a web browser:\n$ open 'http://localhost:7101/api/v1/graph?q=name,randomValue,:eq,:sum,(,name,),:by'","title":"Running Demo with Memory Storage"},{"location":"/concepts.html","text":"","title":"Concepts"},{"location":"/concepts.html#concepts","text":"This page is meant to provide a basic overview of some of the key concepts and terminology that is used for Atlas. If you have experience with other monitoring systems, then much of this should be familiar though we might be using different terms.","title":"Concepts"},{"location":"/concepts.html#time-series","text":"Atlas is a backend for storing and querying dimensional time series data. A time series is a sequence of data points reported at a consistent interval over time. The time interval between successive data points is called the step size. In Atlas each time series is paired with metadata called tags that allow us to query and group the data.","title":"Time Series"},{"location":"/concepts.html#tags","text":"A set of key value pairs associated with a time series. Each time series must have at least one tag with a key of name. To make it more concrete, here is an example of a tag set represented as a JSON object:\n{\n  \"name\":       \"server.requestCount\",\n  \"status\":     \"200\",\n  \"endpoint\":   \"api\",\n  \"nf.app\":     \"fooserver\",\n  \"nf.cluster\": \"fooserver-main\",\n  \"nf.stack\":   \"main\",\n  \"nf.region\":  \"us-east-1\",\n  \"nf.zone\":    \"us-east-1c\",\n  \"nf.node\":    \"i-12345678\"\n}\nUsage of tags will typically fall into two categories:\nNamespace: these are tags necessary to qualify a name so it can be meaningfully aggregated. For example, using the sample above consider computing the sum of all metrics for application fooserver. That number would be meaningless. Properly modelled data should try to make the aggregates meaningful by selecting the name. The sum of all metrics with name = server.requestCount is the overall request count for the service. Dimensions: these are tags used to drill down into the data, for example, see the number of requests with status = 200 or for a particular node. Most tags should fall into this category.\nWhen creating metrics it is important to carefully think about how the data should be tagged. See the naming conventions docs for more information.","title":"Tags"},{"location":"/concepts.html#metric","text":"A metric is a specific quantity being measured, e.g., the number of requests received by a server. In casual language about Atlas metric is often used interchangeably with time series. A time series is one way to track a metric and is the method supported by Atlas. In most cases there will be many time series for a given metric. Going back to the example, request count would usually be tagged with additional dimensions such as status and node. There is one time series for each distinct combination of tags, but conceptually it is the same metric.","title":"Metric"},{"location":"/concepts.html#data-point","text":"A data point is a triple consisting of tags, timestamp, and a value. It is important to understand at a high level how data points correlate with the measurement. Consider requests hitting a server, this would typically be measured using a counter. Each time a request is received the counter is incremented. There is not one data point per increment, a data point represents the behavior over a span of time called the step size. The client library will sample the counter once for each interval and report a single value.\nSuppose that each circle in the diagram below represents a request:\n1:00       1:01       1:02       1:03\n ├─●────●●●─┼──────────┼──●───────┤\nThere are 5 requests shown, 4 from 1:00 to 1:01, and 1 from 1:02 to 1:03. Assuming all requests incremented the same time series, i.e. all other dimensions such as status code are the same, then this would result in three data points. For counters values are always a rate per second, so for a one minute step size the total number of requests would be divided by 60 seconds. So the values stored would be:\nTime Value 1:01 4 / 60 = 0.0667 1:02 0 / 60 = 0.0000 1:03 1 / 60 = 0.0167","title":"Data Point"},{"location":"/concepts.html#step-size","text":"The amount of time between two successive data points in a time series. For Atlas the datapoints will always be on even boundaries of the step size. If data is not reported on step boundaries, it will get normalized to the boundary.","title":"Step Size"},{"location":"/concepts.html#normalization","text":"In Atlas this usually refers to normalizing data points to step boundaries. Suppose that values are actually getting reported at 30 seconds after the minute instead of exactly on the minute. The values will get normalized to the minute boundary so that all time series in the system are consistent.\nHow a normalized value is computed depends on the data source type. Atlas supports three types indicated by the value of the atlas.dstype tag. In general, you should not need to worry about that, client libraries like spectator will automatically handle tagging based on the data source type.\nIt is recommended to at least skim through the normalization for gauges and rates to better understand how the values you see actually relate to measured data.","title":"Normalization"},{"location":"/concepts.html#gauge","text":"A value that is sampled from some source and the value is used as is. The last value received will be the value used for the interval. For example:\n┌─────────┐                                    ┌─────────┐\n                │    8    │                                    │    8    │\n                │         ├───────                             │         │\n                │         │    6                               │         │\n──────┐         │         │                ┌─────────┐         │         │\n 4    │         │         │                │    4    │         │         │\n      ├─────────┤         │           to   │         ├─────────┤         │\n      │    2    │         │                │         │    2    │         │\n ├─────────┼─────────┼─────────┤           ├─────────┼─────────┼─────────┤\n1:00      1:01      1:02      1:03        1:00      1:01      1:02      1:03","title":"Gauge"},{"location":"/concepts.html#rate","text":"A rate is a value representing the rate per second since the last reported value. Rate values are normalized using a weighted average. For example:\n┌─────────┐\n                │    8    │                                    ┌─────────┐\n                │         ├───────                             │    7    │\n                │         │    6                     ┌─────────┤         │\n──────┐         │         │                          │    5    │         │\n 4    │         │         │                ┌─────────┤         │         │\n      ├─────────┤         │           to   │    3    │         │         │\n      │    2    │         │                │         │         │         │\n ├─────────┼─────────┼─────────┤           ├─────────┼─────────┼─────────┤\n1:00      1:01      1:02      1:03        1:00      1:01      1:02      1:03\nIn this example, the data is reported at exactly 30s after the minute boundary. So each value represents the average rate per second for 50% of the minute.\nTime Value 1:01 4 * 0.5 + 2 * 0.5 = 2 + 1 = 3 1:02 2 * 0.5 + 8 * 0.5 = 1 + 4 = 5 1:03 8 * 0.5 + 6 * 0.5 = 4 + 3 = 7\nIf many samples are received for a given interval, then they will each be weighted based on the fraction of the interval they represent. When no previous sample exists, value will be treated as the average rate per second over the previous step. This behavior is important to avoid under counting the contribution from a previous interval. The example below shows what happens if there is no previous or next sample:\n┌─────────┐\n                │    8    │\n                │         │\n                │         │                          ┌─────────┐\n                │         │                          │    5    ├─────────┐\n                │         │                          │         │    4    │\n      ┌─────────┤         │           to        1    │         │         │\n      │    2    │         │                ┌─────────┤         │         │\n ├─────────┼─────────┼─────────┤           ├─────────┼─────────┼─────────┤\n1:00      1:01      1:02      1:03        1:00      1:01      1:02      1:03\nWhy perform weighted averaging for rates instead of the simpler last value approach used with gauges? Because it gives us a better summary of what we actually know from the measurements received. In practical terms:\nIt avoids dropping information if samples are more frequent than the step. Suppose we have a 1 minute step, but data is actually betting reported every 10s. For this example, assume we get 1, 5, 90, 5, 4, and 2. The last value normalization used with gauges would end up with a value of 2. The rate normalization will give 17.833. Each value is a rate per second, so if you take the (1 + 5 + 90 + 5 + 4 + 2) * 10 = 1070 actual events measured during the interval. That is equivalent to 17.833 * 60 indicating we have an accurate average rate for the step size. Avoids skewing the data causing misleading spikes or drops in the aggregates. Using Atlas you will typically be looking at an aggregate of time series rather than an individual time series that was reported. With last value it can have the effect of skewing samples to a later interval. Suppose the client is reporting once a minute at 5s after the minute. That value indicates more about the previous interval than it does the current one. During traffic transitions, such as moving traffic over to a new cluster or even some auto-scaling events, differences in this skew can result in the appearance of a drop because there will be many new time series getting reported with a delayed start. For existing time series it is still skewed, but tends to be less noticeable. The weighted averaging avoids these problems for the most part.","title":"Rate"},{"location":"/concepts.html#counter","text":"Counter is similar to rate, except that the value reported is monotonically increasing and will be converted to a rate by the backend. The conversion is done by computing the delta between the current sample and the previous sample and dividing by the time between the samples. After that it is the same as a rate.\nNote, that unless the input is a montonically increasing counter it is generally better to have the client perform rate conversion. Since, the starting value is unknown, at least two samples must be received before the first delta can be computed. This means that new time series relying on counter type will be delayed by one interval.","title":"Counter"},{"location":"/concepts.html#aggregation-function","text":"Combines a set of time series into a single time series. A conceptual measurement such as cpu usage or requests per second. A metric will usually end up getting reported as a set of time series, for example a cpu usage time series for each node.","title":"Aggregation Function"},{"location":"/concepts.html#consolidation-function","text":"Converts a time series from a primary step size to a larger step size that is a multiple of the primary. For example, if you have a time series with data points each minute and you want a time series with data points every 5 minutes.\nIn most cases you won’t use a consolidation function directly. The system will automatically apply consolidation to ensure there is at least one pixel per datapoint on the rendered images.","title":"Consolidation Function"},{"location":"/concepts.html#nan-aware-operations","text":"This typically refers to how binary operations treat NaN values in the data. A NaN value means that the value is unknown for a given interval. NaN aware operations will keep the value as NaN if both sides are NaN, but treat it as a special value such as 0.0 otherwise. This is useful to avoid gaps in the graph if a subset of time series used in an aggregate have gaps in data.\nlhs rhs result NaN NaN op(NaN, NaN) v1 NaN op(v1, 0.0) NaN v2 op(0.0, v2) v1 v2 op(v1, v2)","title":"NaN Aware Operations"},{"location":"/concepts.html#naming-conventions","text":"See naming conventions.","title":"Naming Conventions"},{"location":"/presentations.html","text":"","title":"Presentations"},{"location":"/presentations.html#presentations","text":"","title":"Presentations"},{"location":"/presentations.html#atlas-overview","text":"High level overview of Atlas backend.\nPresenter @brharrington Date 2014-12-16 Links PDF","title":"Atlas Overview"},{"location":"/presentations.html#atlas-telemetry-a-platform-begets-an-ecosystem","text":"Summary of Atlas and surrounding tooling used at Netflix.\nPresenter @copperlight Date 2015-11-17 Links Video","title":"Atlas Telemetry: A Platform Begets an Ecosystem"},{"location":"/presentations.html#canary-analyze-all-the-things","text":"Overview of canary analysis at Netflix. Canary analysis is one type of tooling that relies heavily on Atlas data internally.\nPresenter @royrapoport Date 2014-08-28 Links Video and Slides","title":"Canary Analyze All The Things"},{"location":"/presentations.html#cloud-operations-at-netflix","text":"Summary of how Netflix thinks about operations. Not specific to Atlas, but Atlas is one of the tools involved.\nPresenter @royrapoport Date 2013-11-20 Links Video","title":"Cloud Operations at Netflix"},{"location":"/presentations.html#monitoring-monitoring-systems-at-netflix","text":"Monitorama 2017 presentation on how we monitor the health of our monitoring systems.\nPresenter @royrapoport Date 2017-05-22 Links Video","title":"Monitoring Monitoring Systems at Netflix"},{"location":"/presentations.html#netflix-amazon-s3-and-amazon-elastic-mapreduce-to-monitor-at-gigascale","text":"AWS re:Invent presentation on the usage of S3 and EMR for Atlas.\nPresenter @royrapoport Date 2013-10-26 Links Video","title":"Netflix: Amazon S3 and Amazon Elastic MapReduce to Monitor at Gigascale"},{"location":"/presentations.html#operational-insight","text":"Overview of operational insight at Netflix.\nPresenter @royrapoport Date 2015-06-17 Links Video","title":"Operational Insight"},{"location":"/asl/index.html","text":"","title":"Stack Language"},{"location":"/asl/index.html#stack-language","text":"Atlas Stack Language is designed to be a stable method of representing complex data queries in a URL-friendly format. It is loosely based on the RPN expressions supported by Tobias Oetiker’s rrdtool. The following is an example of a stack language expression:\nnf.cluster,discovery,:eq,(,nf.zone,),:by\nThis example pushes two strings nf.cluster and discovery onto the stack and then executes the command :eq. The equal command pops two strings from the stack and pushes a query object onto the stack. The behavior can be described by the stack effect String:key String:value – Query. We then push a list of tag keys to the stack and execute the command :by to group the results.","title":"Stack Language"},{"location":"/asl/index.html#parts","text":"There are only four reserved symbols used for structuring the expression: ,:()\nCommas separate items on the stack. So a,b puts two strings on the stack with values \"a\" and \"b\". Colon is used to prefix operations. If the first character is a colon the item will be treated as a command to run. For example, a,:dup, will push \"a\" on the stack and then execute the duplicate operation. Parenthesis are used to indicate the start and end of a list. The expression (,) puts an empty list on the stack. Commands inside of a list will not be executed unless the list is passed to the call command. For example, (,:dup,) will push a list with a single string value of \":dup\" on to the stack.","title":"Parts"},{"location":"/asl/index.html#data-model","text":"The stack language is primarily used for representing expressions over tagged time series data. A tag is a string key value pair used to describe a measurement. Atlas requires at least one tag with a key of name. Example tags represented as a JSON map:\n{\n  \"name\":       \"jvm.gc.pause\",\n  \"cause\":      \"Allocation_Failure\",\n  \"statistic\":  \"count\",\n  \"nf.app\":     \"www\",\n  \"nf.cluster\": \"www-main\",\n  \"nf.asg\":     \"www-main-v001\",\n  \"nf.stack\":   \"main\",\n  \"nf.node\":    \"i-01\",\n  \"nf.region\":  \"us-east-1\",\n  \"nf.zone\":    \"us-east-1a\"\n}\nTypically tags should be dimensions that allow you to use the name as a pivot and other tags to drill down into the data. The tag keys are similar to columns in a traditional table, however, it is important to note that not all time series will have the same set of tag keys.\nThe tags are used to identify a time series, which conceptually is a set of timestamp value pairs. Here is a simplified data set shown as a table:\nname app node values cpuUsage www i-01 [(05:00, 33.0), (05:01, 31.0)] cpuUsage www i-02 [(05:00, 20.0), (05:01, 37.0)] cpuUsage db i-03 [(05:00, 57.0), (05:01, 62.0)] diskUsage www i-01 [(05:00, 9.0), (05:01, 9.0)] diskUsage www i-02 [(05:00, 7.0), (05:01, 8.0)] requestRate www [(05:00, 33.0), (05:01, 31.0)]\nThe table above will be used for the examples in later sections.","title":"Data Model"},{"location":"/asl/index.html#simple-expressions","text":"All expressions generally have four parts:\nQuery: selects a set of time series. Aggregation: defines how to combine the selected time series. Math: manipulate the time series values or combine aggregated results with binary operations. Presentation: adjust how the data is presented in a chart.","title":"Simple Expressions"},{"location":"/asl/index.html#query","text":"The query is used to select a set of time series. The primary query operators are :eq and :and. For a full list see the query section of the reference page.\nSample query to select all time series where the key node is equal to i-01:\nnode,i-01,:eq\nIf you are familiar with SQL and assume that tag keys are column names, then this would be equivalent to:\nselect * from time_series where node = 'i-01';\nUsing the example data set this query would return the following subset:\nname app node values cpuUsage www i-01 [(05:00, 33.0), (05:01, 31.0)] diskUsage www i-01 [(05:00, 9.0), (05:01, 9.0)]\nTo get just the cpu usage for that node, use :and:\nnode,i-01,:eq,name,cpuUsage,:eq,:and\nThis would result in:\nname app node values cpuUsage www i-01 [(05:00, 33.0), (05:01, 31.0)]","title":"Query"},{"location":"/asl/index.html#aggregation","text":"An aggregation function maps a set of time series that matched the query to a single time series. Atlas supports four aggregate functions: sum, min, max, and count. If no aggregate is specified on an expression, then sum will be used implicitly.\nUsing the example data set, these two expressions would be equivalent:\napp,www,:eq,name,cpuUsage,:eq,:and\napp,www,:eq,name,cpuUsage,:eq,:and,:sum\nAnd would result in a single output time series:\nname app values cpuUsage www [(05:00, 53.0), (05:01, 68.0)]\nNote that the node is not present in the output. The set of tags on the output will be ones with exact matches in the query clause or explicitly listed in the group by.\nIf you wanted the max cpu for the application, then you would write:\napp,www,:eq,name,cpuUsage,:eq,:and,:max\nWhat if we want the average? The count aggregate is used to determine how many time series had a value for a given time. To get the average we divide the sum by the count.\napp,www,:eq,name,cpuUsage,:eq,:and,\n:dup,\n:sum,\n:swap,\n:count,\n:div\nThere is a helper macro :avg that will do this for you, so you can write:\napp,www,:eq,name,cpuUsage,:eq,:and,:avg","title":"Aggregation"},{"location":"/asl/index.html#group-by","text":"In many cases we want to group the results that were selected and return one aggregate per group. As an example suppose I want to see maximum cpu usage by application:\nname,cpuUsage,:eq,:max,(,app,),:by\nUsing the example data set, this would result in a two output time series:\nname app values cpuUsage www [(05:00, 33.0), (05:01, 37.0)] cpuUsage db [(05:00, 57.0), (05:01, 62.0)]","title":"Group By"},{"location":"/asl/index.html#math","text":"Once you have a set of lines, it can be useful to manipulate them. The supported operations generally fall into two categories: unary operations to alter a single time series and binary operations that combine two time series.\nExamples of unary operations are negate and absolute value. To apply the absolute value:\napp,web,:eq,name,cpu,:eq,:and,:sum,:abs\nMultiple operations can be applied, for example, negating the line then applying the absolute value:\napp,web,:eq,name,cpu,:eq,:and,:sum,:neg,:abs\nCommon binary operations are add, subtract, multiply, and divide. The aggregation section has an example of using divide to compute the average.\nFor a complete list see the math section of the reference page.","title":"Math"},{"location":"/asl/index.html#presentation","text":"Once you have a final expression, you can apply presentation settings to alter how a time series is displayed in the chart. One of the most common examples is setting the label to use for the legend:\napp,www,:eq,name,cpuUsage,:eq,:and,:avg,\naverage cpu usage,:legend\nYou can also use tag keys as variables in the legend text, for example, setting the legend to the application:\napp,www,:eq,name,cpuUsage,:eq,:and,:avg,(,app,),:by,\n$(app),:legend\nIt is also common to adjust the how the lines are shown. For example, to stack each of the lines we can use the :stack command to adjust the line style:\napp,www,:eq,name,cpuUsage,:eq,:and,:avg,(,app,),:by,\n:stack,\n$(app),:legend\nFor a complete list see the style section of the reference page.","title":"Presentation"},{"location":"/asl/alerting.html","text":"","title":"Alerting Expressions"},{"location":"/asl/alerting.html#alerting-expressions","text":"The stack language provides some basic techniques to convert an input line into a set of signals that can be used to trigger and visualize alert conditions. This section assumes a familiarity with the stack language and the alerting philosophy","title":"Alerting Expressions"},{"location":"/asl/alerting.html#threshold-alerts","text":"To start we need an input metric. For this example the input will be a sample metric showing high CPU usage for a period:\n/api/v1/graph?s=e-3h&e=2012-01-01T07:00&tz=UTC&l=0&h=100&q=nf.app,alerttest,:eq,name,ssCpuUser,:eq,:and,:sum\nLets say we want to trigger an alert when the CPU usage goes above 80%. To do that simply use the :gt operator and append 80,:gt to the query:\n/api/v1/graph?s=e-3h&e=2012-01-01T07:00&tz=UTC&l=0&h=100&q=nf.app,alerttest,:eq,name,ssCpuUser,:eq,:and,:sum,80,:gt\nThe result is a signal line that is non-zero, typically 1, when in a triggering state and zero when everything is fine.","title":"Threshold Alerts"},{"location":"/asl/alerting.html#dampening","text":"Our threshold alert above will trigger if the CPU usage is ever recorded to be above the threshold. Alert conditions are often combined with a check for the number of occurences. This is done by using the :rolling-count operator to get a line showing how many times the input signal has been true withing a specified window and then applying a second threshold to the rolling count.\nInput Rolling count Dampened signal \nnf.app,alerttest,:eq,\nname,ssCpuUser,:eq,\n:and,\n:sum,\n80,:gt\n \nnf.app,alerttest,:eq,\nname,ssCpuUser,:eq,\n:and,\n:sum,\n80,:gt,\n5,:rolling-count\n \nnf.app,alerttest,:eq,\nname,ssCpuUser,:eq,\n:and,\n:sum,\n80,:gt,\n5,:rolling-count,\n4,:gt","title":"Dampening"},{"location":"/asl/alerting.html#visualization","text":"A signal line is useful to tell whether or not something is in a triggered state, but can be difficult for a person to follow. Alert expressions can be visualized by showing the input, threshold, and triggering state on the same graph.\n/api/v1/graph?s=e-3h&e=2012-01-01T07:00&tz=UTC&l=0&h=100&q=nf.app,alerttest,:eq,name,ssCpuUser,:eq,:and,:sum,80,:2over,:gt,:vspan,40,:alpha,triggered,:legend,:rot,input,:legend,:rot,threshold,:legend,:rot","title":"Visualization"},{"location":"/asl/alerting.html#summary","text":"You should now know the basics of crafting an alert expression using the stack language. Other topics that may be of interest:\nAlerting Philosophy: overview of best practices associated with alerts. Stack Language Reference: comprehensive list of avialable operators. DES: double exponential smoothing. A technique for detecting anomalies in normally clean input signals where a precise threshold is unknown. For example, the requests per second hitting a service.","title":"Summary"},{"location":"/asl/alerting-philosophy.html","text":"","title":"Alerting Philosophy"},{"location":"/asl/alerting-philosophy.html#alerting-philosophy","text":"It is recommended for all alerts to adhere to the follow guidelines:\nKeep conditions simple. Alerts should be actionable. Check for measured failure on critical paths rather than a lack of success. Alerts should not have special cases for routine maintenance. Consider how the alert check can fail.","title":"Alerting Philosophy"},{"location":"/asl/alerting-philosophy.html#keep-it-simple","text":"When an alert triggers, it should be easy to understand why. Similarly, if an alert doesn’t fire, then it should be easy to check and see what happened. The more complicated an alert condition becomes, the harder it is to understand and debug.\nIt is recommended to keep alert rules as a simple expression with a threshold and number of occurrences. An example of this is the following rule:\nCPU Usage > 80% for at least 5 minutes\nMultiple signals should only be combined if it improves the effectiveness of the alert. For example, what is an appropriate threshold for the number of requests that have error responses? What happens to that threshold if your cluster auto-scales? It is more effective to define the threshold as a percentage of total requests:\n(Num Errors / Num Total) > 0.01 for at least 5 minutes\nIn some cases, a low volume can make the percentages less meaningful and result in false positives. For example, if your daily traffic pattern follows a sine curve, then the troughs may not represent a meaningful error percentage. Another example might be during failover exercises, if traffic has been failed over to another cluster. One way to compensate for this is to check the failure rate and overall volume:\nPercentage of Failures > X AND Volume > Y\nAs a general rule, bias towards simplicity. If you are creating more complex expressions, then stop and think about why that complexity is needed. Are there other signals available that are easier to use? Can the application be changed so that it reports metrics which make it easier to diagnose?","title":"Keep It Simple"},{"location":"/asl/alerting-philosophy.html#actionable-alerts","text":"If an alert fires and sends a notification to users, someone should be motivated to investigate the problem. Alerts that are noisy or not actionable train people to ignore or filter out alert notifications.\nFor cases where the response to an alert can be automated, such as terminating a bad instance, it shouldn’t send out a notification unless there is a failure to perform the action. If you want a summary of cluster health, then use dashboards or reporting tools for this function; don’t attempt to do this via alert notifications.\nAlerts should check something important. To setup effective alerts, you need to understand the application and have ways to detect failures for critical functionality. Avoid general system-type alerts that won’t be investigated. For example, should you alert on high CPU usage? If you have done squeeze testing and you have information to indicate how CPU usage impacts the application, then it can be useful and it will provide a way to know a problem is coming before it impacts clients of the service. If you do not have this knowledge, then your alert may be under-tuned, leading to noisy notifications that may be ignored.","title":"Actionable Alerts"},{"location":"/asl/alerting-philosophy.html#check-for-measured-failure","text":"It is better to check for failures rather than trying to trigger based on an absence of information or a reduction in the amount of success.","title":"Check for Measured Failure"},{"location":"/asl/alerting-philosophy.html#absence-of-information","text":"A typical example of this is a process that runs over a longer time period. For example, suppose we have an application that updates a metadata cache once per day and it takes an hour to refresh. It is not recommended to send an event on refresh success and then configure alerts based on the absence of the success event. Design the signals so you have a clear way to understand what error conditions may be occurring on and then alert if there is a problem.\nIn this example, a better design would use a gauge that reports the loading time and a gauge that reports the age of the cache. You can then add alerts when the gauges for these error conditions exceed unacceptable thresholds.","title":"Absence of Information"},{"location":"/asl/alerting-philosophy.html#reduction-in-success","text":"Let’s say we have a server that is taking traffic and we want to know if users are experiencing problems. How should we go about this? It is often tempting to look for things like a drop in the number of successful requests, because this can be a generic catch-all for many types of problems.\nHowever, alerts of this sort are inherently noisy. How do you know what the number of requests should be? While there are various schemes for trying to predict the behavior, you will spend a lot of time tuning alerts of this nature to get them to the point where they are not too noisy, but they still catch real issues. Further, these schemes cannot differentiate between problems for the service and unrelated drops such as a client having problems and failing to make the request in the first place.\nIf you’re not going to investigate these alerts when they fire or invest in tuning and maintaining them, just avoid this type of alert altogether.\nA better approach is to alert on the number of failures you are seeing from a service. Thresholds can often be determined automatically by looking at the percent of all requests that are failures. For middle tier services, it is also likely that data from the clients can be used to see a percentage of failure from a client perspective instead of, or in addition to, the server side view.","title":"Reduction in Success"},{"location":"/asl/alerting-philosophy.html#avoid-special-cases","text":"Alerts shouldn’t have to be tuned or suppressed during regular maintenance such as replacing instance or doing deployments. As a simple example, consider an alert on the rate of failures. The general assumption would be that a deployment should not be noticed by clients and therefore the alert is still relevant. Alerts that are actionable and look for measured failure tend to work well. If a new instance is coming up, a lack of activity will mean a lack of failures until traffic is being received. At that time if there are failures they should be noticed.","title":"Avoid Special Cases"},{"location":"/asl/alerting-philosophy.html#startup-behavior","text":"What about different behavior during startup? Consider some examples for an application that has a long initialization time (~20 minutes) before it can take traffic:\nDiscovery service state during initialization. Healthcheck failures during initialization. Performance may be different while starting. CPU usage is high while initializing but stabilizes and remains low during normal operation.\nFor a discovery service like Eureka, the duration of the startup time shouldn’t be an issue because the state clearly indicates if it is STARTING vs DOWN.\nIf the healthcheck is used for a load balancer, then the decision to send traffic to instances should be fairly sensitive in order to minimize the impact to users. The bigger concern is the number of occurrences of healthcheck failures in a row, which can trigger automated actions like terminating an instance. When evaluating healthcheck failures, there are two distinct conditions to evaluate: non-200 responses and connection timeouts.\nThe healthcheck logic should be tied to the Eureka heartbeat so that if the healthcheck is failing due to a non-200 response, the discovery state will be DOWN after initialization is complete. For the first condition, the alert should check for the number of occurrence of the DOWN state in the discovery service which will not trigger for the STARTING state used during application initialization.\nFor the second condition, you would need to check for a disparity between the published discovery state and the healthcheck state:\n(DiscoveryStatus is UP) AND (Healthcheck != 200) for N minutes\nNote, unless you really need to do this it is probably better to just look at the healthcheck and have the num occurrences set to be longer than the typical startup time.\nFor the CPU example, first reconsider whether general system check alerts are actually useful. Is it going to help you catch a real problem and be investigated when it triggers? If not, don’t setup an alert on CPU and rely on alerts that check for failures on the critical path.\nIf it is useful and you have squeeze testing results or other information so you know when a proxy metric like CPU actually indicates a problem, then you can configure it restricted with some signal that indicates the status. However, keep in mind that not all systems will allow complex expressions. For example, if you are auto-scaling will you be able to send the data such that it doesn’t incorrectly skew the alarm? The more signals that are combined the harder it is to understand the alert and the more likely it is to fail in unexpected ways. Before adding more layers of duct tape think hard about the application and if you can change it to be easier to monitor and diagnose.","title":"Startup Behavior"},{"location":"/asl/alerting-philosophy.html#deployments","text":"At Netflix, a common deployment model is red/black. In this model, a new auto-scaling group the same size as the existing one will be created, traffic will transition over, and eventually the old auto-scaling group (ASG) will be deleted. This can create false alarms if you haven’t thought about the signals being used to fire alerts.\nThe most common alerting problem that occurs during deployments is related the use of averages. For example, the average request rate will drop in half if a new ASG comes up and you are aggregating across a cluster consisting of both old and new ASGs. If you follow the advice given earlier about crafting alerts based on a percentage of errors reported by clients of the application, then aggregating across clusters by sum usually won’t be a problem. If the deployment is going well, then the overall failure rate seen by clients shouldn’t be impacted.\nAnother example of a deployment alerting problem is latency measurements. How can you tell the average latency across a cluster composed of new and old ASGs? Rather than trying to special case or exclude the new group of instances, you should define the alert signal based on the actual activity seen. If there is no activity within an ASG, then it will not impact the signal.\nMetrics libraries like Spectator send both a totalTime and count measurement separately to the backend. This allows the average to be computed using a simple sum aggregate with division:\nSum(totalTime per instance in cluster) / Sum(count per instance in cluster)\nThis calculation demonstrates how instances that are not receiving traffic will not contribute anything to the sums.","title":"Deployments"},{"location":"/asl/alerting-philosophy.html#think-about-failure","text":"An effective alert needs to be able to fire when there is a problem. However, when problems occur, it is possible that the problem will also impact the underlying data or mechanisms used to detect issues for the alert. It is worthwhile to spend time thinking about the ways in which your alerts can fail to detect events.","title":"Think About Failure"},{"location":"/asl/alerting-philosophy.html#how-can-signals-fail-","text":"The simplest area to think about is what is collecting and reporting the data. For example, if data is being reported by the plugin running in the application, then it won’t work if the application crashes or cannot start. It is recommended to have some basic alerts using a data pipeline that will fail independently from the application. At Netflix, this typically involves checking the following conditions:\nThe healthcheck is responding with 200. This signal indicates that a remote system was able to connect and query the application healthcheck. So the application is running and inbound traffic made it in. The application is registered with Eureka. Eureka uses a heartbeat mechanism, so checking the registration tells you the application is running and it is able to successfully send the heartbeat request.\nThe metric data for those signals comes from a separate poller application. If these succeed, then the application should be healthy enough that alerts triggered from data local to the instance should be working.","title":"How Can Signals Fail?"},{"location":"/asl/alerting-philosophy.html#alerting-scopes","text":"At Netflix, alert expressions for Atlas can be checked in three places:\nBackend. Alerts checked against the main backend server. Plugin. Alerts are checked by the plugin running on the instance. Poller. Alerts are checked by a poller service that collects data about instances.\nIn practice, for a given application, the alerting scopes look like:\nAlerting scopes can be used to provide some level of redundancy with different failure modes. For example, the failure rate could be checked against the server stats and the client stats. Further, it is recommended to check alerts as close as possible to where the data is initially measured and collected. In other words, it is better to check the alerts on the plugin or poller rather than against the backend. The advantages of doing this are:\nLower mean-time to detection (MTTD). Data going to the backend server has to be handled by several layers, which need to allow time for data from all nodes to arrive, time to index, etc. Alerts checked locally using the plugin or poller will get checked as data is being published and so they can trigger at the same time that data would hit the first step in the backend data pipeline. More robust to failure. When there are problems with the monitoring backends, server side alerts won’t work or may have incorrect or partial data. Alerts checked locally on the plugin are immune to all problems off the instance other than being able to forward to the alert server. If the Atlas plugin or the instance running it are having issues, then it is likely that problems for the local alert check would also impact publishing, so server side alerts are not likely to provide a better view. Also, keep in mind that for most middle-tier services, the alert can be checked on the instances that call the service and thus can still fire if the instance has crashed. High-level instance health can be verified by an alert checked on the poller. Scales better as the amount of data and number of alerts increases. Many alerts, in particular if checked per node, require expensive queries to run on the backends. By checking alert using the plugin, the computation is spread out so each node is checking the alerts for that instance.\nSo why not check all alerts on the client or poller? The primary disadvantages:\nClient and poller scopes can only use data that is available at that location. For a client, that means only the data that is reported by the plugin on that instance. For the poller, it means only data about health checks, discovery, and system stats from SNMP. Data cannot be aggregated across nodes for the cluster. This can make it harder to do things like outlier detection using a cluster-level aggregate as a baseline. However, keep in mind that for middle-tier services there is often an option to check on the plugin for the client.","title":"Alerting Scopes"},{"location":"/asl/des.html","text":"","title":"Double Exponential Smoothing"},{"location":"/asl/des.html#double-exponential-smoothing","text":"TODO","title":"Double Exponential Smoothing"},{"location":"/asl-reference/index.html","text":"","title":"Stack Language Reference"},{"location":"/asl-reference/index.html#stack-language-reference","text":"","title":"Stack Language Reference"},{"location":"/asl-reference/query/index.html","text":"","title":"Query"},{"location":"/asl-reference/query/index.html#query","text":"Query expression used to select a set of time series. For more information see the stack language tutorial.","title":"Query"},{"location":"/asl-reference/query/and.html","text":"","title":"and"},{"location":"/asl-reference/query/and.html#and","text":"","title":"and"},{"location":"/asl-reference/and.html","text":"","title":"and"},{"location":"/asl-reference/and.html#and","text":"There are two overloaded variants of the :and operator:\nQuery variant that is used to help restrict the set of time series used for an expression. Math variant that is used to combine to signal time series.","title":"and"},{"location":"/asl-reference/dist-avg.html","text":"","title":"dist-avg"},{"location":"/asl-reference/dist-avg.html#dist-avg","text":"","title":"dist-avg"},{"location":"/asl-reference/dist-avg.html#signature","text":"Query -- TimeSeriesExpr","title":"Signature"},{"location":"/asl-reference/dist-avg.html#summary","text":"Compute standard deviation for timers and distribution summaries.","title":"Summary"},{"location":"/asl-reference/dist-avg.html#examples","text":"","title":"Examples"},{"location":"/asl-reference/dist-avg.html#name-playback-startlatency-eq-dist-stddev","text":"Pos Input Output 0","title":"name,playback.startLatency,:eq,:dist-stddev"},{"location":"/asl-reference/math-and.html","text":"","title":"and"},{"location":"/asl-reference/math-and.html#and","text":"This is a sample graph:","title":"and"},{"location":"/spectator/index.html","text":"","title":"Spectator"},{"location":"/spectator/index.html#spectator","text":"Atlas client library.","title":"Spectator"},{"location":"/spectator/timers.html","text":"","title":"Timers"},{"location":"/spectator/timers.html#timers","text":"A timer is used to measure how long some event is taking. Two types of timers are supported:\nTimer: for frequent short duration events. LongTaskTimer: for long running tasks.\nThe long duration timer is setup so that you can track the time while an event being measured is still running. A regular timer just records the duration and has no information until the task is complete.\nAs an example, consider a chart showing request latency to a typical web server. The expectation is many short requests so the timer will be getting updated many times per second.\nNow consider a background process to refresh metadata from a data store. For example, Edda caches AWS resources such as instances, volumes, auto-scaling groups etc. Normally all data can be refreshed in a few minutes. If the AWS services are having problems it can take much longer. A long duration timer can be used to track the overall time for refreshing the metadata.\nThe charts below show max latency for the refresh using a regular timer and a long task timer. Regular timer, note that the y-axis is using a logarithmic scale:\nLong task timer:","title":"Timers"},{"location":"/spectator/timers.html#timer","text":"To get started create an instance using the registry:\nJava public class TimerExample {\n\n  private final Registry registry;\n  private final Timer requestLatency;\n\n  @Inject\n  public TimerExample(Registry registry) {\n    this.registry = registry;\n    requestLatency = registry.timer(\"server.requestLatency\");\n  } Python class TimerExample:\n\n    def __init__(self, registry=GlobalRegistry):\n        self._registry = registry\n        self._requestLatency = registry.timer('server.requestLatency')\nThen wrap the call you need to measure, preferably using a lambdaa with statement:\nJava public Response handleUsingLambda(Request request) throws Exception {\n  return requestLatency.record(() -> handleImpl(request));\n} Python def handle_using_with(self, request):\n    with self._requestLatency.stopwatch():\n        return self._handle_impl(request)\nThe lambda variants will handle exceptions for you and ensure the record happens as part of a finally block using the monotonic time. It could also have been done more explicitly like:\nJava public Response handleExplicitly(Request request) {\n  final long start = registry.clock().monotonicTime();\n  try {\n    return handleImpl(request);\n  } finally {\n    final long end = registry.clock().monotonicTime();\n    requestLatency.record(end - start, TimeUnit.NANOSECONDS);\n  }\n} Python def handle_explicitly(self, request):\n    start = self._registry.clock().monotonic_time()\n    try:\n        return self._handle_impl(request)\n    finally:\n        end = self._registry.clock().monotonic_time()\n        self._requestLatency.record(end - start)\nThis example uses the clock from the registry which can be useful for testing if you need to control the timing. In actual usage it will typically get mapped to the system clock. It is recommended to use a monotonically increasing source for measuring the times to avoid occasionally having bogus measurements due to time adjustments. For more information see the Clock documentation.","title":"Timer"},{"location":"/spectator/timers.html#longtasktimer","text":"To get started create an instance using the registry:\npublic class MetadataService {\n\n  private final LongTaskTimer metadataRefresh;\n\n  @Inject\n  public MetadataService(Registry registry) {\n    metadataRefresh = registry.longTaskTimer(\"metadata.refreshDuration\");\n    // setup background thread to call refresh()\n  }\n\n  private void refresh() {\n    final int id = metadataRefresh.start();\n    try {\n      refreshImpl();\n    } finally {\n      metadataRefresh.stop(id);\n    }\n  }\nThe id is used to keep track of a particular task being measured by the timer. It must be stopped using the provided id. Note that unlike a regular timer that does not do anything until the final duration is recorded, a long duration timer will report as two gauges:\nduration: total duration spent within all currently running tasks. activeTasks: number of currently running tasks.\nThis means that you can see what is happening while the task is running, but you need to keep in mind:\nThe id is fixed before the task begins. There is no way to change tags based on the run, e.g., update a different timer if an exception is thrown. Being a guage it is inappropriate for short tasks. In particular, gauges are sampled and if it is not sampled during the execution or the sampling period is a significant subset of the expected duration, then the duration value will not be meaningful.\nLike a regular timer, the duration timer also supports using a lambda to simplify the common case:\nprivate void refresh() {\n    metadataRefresh.record(this::refreshImpl);\n  }","title":"LongTaskTimer"},{"location":"/spectator/netflix.html","text":"","title":"Netflix Integration"},{"location":"/spectator/netflix.html#netflix-integration","text":"When running at Netflix, use the atlas-client library to enable transferring the instrumented data to Atlas. See the appropriate section for the type of project you are working on: Libraries Applications, specifically standalone apps using guice or governator directly. Base Server Libraries For libraries, the only dependency that should be needed is: com.netflix.spectator:spectator-api:0.62.0\n The bindings to integrate internally should be included with the application. In your code, just inject a registry, e.g.: public class Foo {\n  @Inject\n  public Foo(Registry registry) {\n    ...\n  }\n  ...\n}\n See the testing docs for more information about creating a binding to use with tests. Libraries should not install SpectatorModule. The bindings to use for the registry should be determined by the application that is using the library. Think of it as being like slf4j where logging configuration is up to the end-user, not the library owner. When creating a Guice module for your library, you may want to avoid binding errors if the end-user has not provided a binding for the Spectator registry. This can be done by using optional injections inside of the module, for example: // Sample library class\npublic class MyLib {\n  Registry registry;\n\n  @Inject\n  public MyLib(Registry registry) {\n    this.registry = registry;\n  }\n}\n\n// Guice module to configure the library and setup the bindings\npublic class MyLibModule extends AbstractModule {\n\n  private static final Logger LOGGER = LoggerFactory.getLogger(MyLibModule.class);\n\n  @Override\n  protected void configure() {\n  }\n\n  @Provides\n  private MyLib provideMyLib(OptionalInjections opts) {\n    return new MyLib(opts.registry());\n  }\n\n  private static class OptionalInjections {\n    @Inject(optional = true)\n    private Registry registry;\n\n    Registry registry() {\n      if (registry == null) {\n        LOGGER.warn(\"no spectator registry has been bound, so using noop implementation\");\n        registry = new NoopRegistry();\n      }\n      return registry;\n    }\n  }\n}\n Applications Application should include a dependency on the atlas-client plugin: netflix:atlas-client:latest.release\n Note this is an internal only library with configs specific to the Netflix environments. It is assumed you are using Nebula so that internal maven repositories are available for your build. When configuring with governator specify the AtlasModule: Injector injector = LifecycleInjector.builder()\n    .withModules(new AtlasModule())\n    .build()\n    .createInjector();\n The registry binding will then be available so it can be injected as shown in the libraries section. The insight libraries do not use any governator or guice specific features. So it is possible to use guice or other dependency injection frameworks directly with the following caveats: However, some of the libraries do use the @PostConstruct and @PreDestroy annotations for managing lifecycle. Governator adds lifecycle management and many other features on top of guice and is the recommended way. For more minimalist support of just the lifecycle annotations on top of guice see iep-guice. The bindings and configuration necessary to run correctly with the internal setup are only supported as guice modules. If trying to use some other dependency injection framework, then you will be responsible for either finding a way to leverage the guice module in that framework or recreating those bindings and maintaining them as things change. It is not a paved road path. Base Server If using base-server, then you will get the Spectator and Atlas bindings automatically. Auto Plugin\nWarning Deprecated: Use of AutoBindSingleton is generally discouraged. It is recommended to use one of the other methods.\nIf you are only interested in getting the GC logging, there is a library with an auto-bind singleton that can be used: com.netflix.spectator:spectator-nflx:0.62.0\n Assuming you are using karyon/base-server or governator with com.netflix in the list of base packages then the plugin should get automatically loaded.\nWhen running at Netflix, use the nflx-spectator-config library to enable transferring the instrumented data to Atlas. See the appropriate section for the type of project you are working on: Libraries Applications Winston Libraries Libraries should use the open source library directly and allow the application to determine how it is configured. pip install netflix-spectator-py\n Applications Should include a dependency on nflx-spectator-config to get the right settings for publishing internally. Winston If using Winston, then you will get the Spectator and Atlas bindings automatically.","title":"Netflix Integration"}]}