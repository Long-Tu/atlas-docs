{"docs":[{"location":"overview.html#overview","title":"Overview","pageTitle":"Overview","text":"Atlas is the system Netflix uses to manage dimensional time-series data for near real-time operational insight. It was primarily created to address issues with scale and query capability in the previous system."},{"location":"overview.html#history","title":"History","pageTitle":"Overview","text":"In May of 2011, Netflix was using a home-grown solution called Epic to manage time-series data. Epic was a combination of perl CGI scripts, RRDTool logging, and MySQL. We were tracking around 2M distinct time series and the monitoring system was regularly failing to keep up with the volume of data. In addition there were a number of trends in the company which presaged a drastic increase in metric volume: Rolling pushes to Red/Black deployments. Leveraging auto-scaling for large clusters. Netflix has always used auto-scaling groups in AWS, but initially most were configured with fixed size and just used as a group and to replace instances. Expansion internationally into Latin America and Europe. This led to an increase in the number of countries being tracked for key metrics and for Europe it was the first move into additional AWS regions. With additional regions we also wanted to have better isolation so a problem with monitoring in one region would not impact another, but at the same time have a mechanism to provide a global view if needed. Since that time the metric volume has continued to grow quickly. The graph below shows the increase in metrics measured over last few years:  The growth in raw volume required increased query capability to actually use the data."},{"location":"overview.html#goals","title":"Goals","pageTitle":"Overview","text":"The main goals for Atlas were to build a system that provided: A Common API Scale Dimensionality"},{"location":"overview.html#common-api","title":"Common API","pageTitle":"Overview","text":"Epic did a number of things really well that we didn’t want to lose when transitioning. In particular: Normalization and consolidation Flexible legends that scale independently of the chart Math, especially handling of NaN values representing no data Holt-Winters used for alerting Visualization options Deep linking Many of these are capabilities that are provided by the RRDTool library Epic was using, but most alternatives we looked at fell short in these categories. In addition, we have uses for other 3rd party services like CloudWatch and it is desirable to have common query capability for that data."},{"location":"overview.html#scale","title":"Scale","pageTitle":"Overview","text":"As indicated in the history section, metrics volume was growing and we needed a system that could keep up. For a long time our biggest concern was write volume, however, we also wanted to scale in terms of the amount of data we could read or aggregate as part of a graph request."},{"location":"overview.html#dimensionality","title":"Dimensionality","pageTitle":"Overview","text":"This is a decision that was made because users were already doing it in ways that were hard to support. Epic only support a simple name with some special case system dimensions of cluster and node. Many users were creating names like: com.netflix.eds.nccp.successful.requests.uiversion.nccprt-authorization.devtypid-101.clver-PHL_0AB.uiver-UI_169_mid.geo-US That breaks down to: Key Value name com.netflix.eds.nccp.successful.requests.uiversion nccprt authorization devtypid 101 clver PHL_0AB uiver UI_169_mid geo US Since it was all mangled into a name with different conventions by team, users would have to resort to complex regular expressions to slice and dice the data based on the dimensions."},{"location":"overview.html#query-layer","title":"Query Layer","pageTitle":"Overview","text":"In order to get a common API, have flexibility for backend implementations, and provide merged views across backends we built a query layer that can be hierarchically composed. The diagram below shows the main Netflix setup:  We have isolated regional deployments in each region we operate in as well as a global deployment that can combine the results from multiple regions. The query and aggregation operations can be performed on the fan out so most of the big summarization operations will distribute the computation across the tree and typically to an optimized storage layer at some point. Allowing the query and rendering layer to work on multiple backends also makes it easier for us to consider transitioning to other backends in the future such as OpenTSDB or InfluxDB. Switching to Atlas one of the biggest hurdles was compatibility and transitioning to the new system."},{"location":"overview.html#stack-language","title":"Stack Language","pageTitle":"Overview","text":"One of our key requirements was to be able to have deep links into a particular chart and to be able to reliably pass around or embed these images via email, wikis, html pages, etc. In addition, the user who receives the link should be able to tweak the result. Atlas uses a simple stack language that has a minimal punctuation and allows arbitrarily complex graph expressions to be encoded in a URL friendly way. This means that all images can be accessed using a GET request. The stack language is also simple to parse and interpret, allowing it to be easily consumed from a variety of tools. The core features include: Embedding and linking using a GET request URL friendly stack language Few special symbols (comma, colon, parenthesis) Easy to extend Basic operations Query: and, or, equal, regex, has key, not Aggregation: sum, count, min, max, group by Consolidation: aggregate across time Math: add, subtract, multiply, etc Boolean: and, or, lt, gt, etc Graph settings: legends, area, transparency"},{"location":"overview.html#graph-example","title":"Graph Example","pageTitle":"Overview","text":"To illustrate, this is a sample graph image:  This graph shows the number of requests per second and compares that with a prediction line generated using double exponential smoothing. If the number of requests per second falls below the prediction, it indicates an alert would trigger using the vertical spans. The url to generate this image follows (newlines added for readability): http://atlas/api/v1/graph\n  ?tz=UTC\n  &e=2012-01-01T08:00\n  &s=e-8h\n  &w=500\n  &h=150\n  &l=0\n  &q=nf.cluster,alerttest,:eq,\n     name,requestsPerSecond,:eq,:and,\n     :sum,\n     :dup,10,0.1,0.02,:des,\n     0.85,:mul,\n     :2over,:lt,\n     :rot,$name,:legend,\n     :rot,prediction,:legend,\n     :rot,:vspan,60,:alpha,alert+triggered,:legend Adding some comments to the stack expression to explain a bit what is going on: -- Query to generate the input line\nnf.cluster,alerttest,:eq,\nname,requestsPerSecond,:eq,:and,\n:sum,\n\n-- Create a copy on the stack\n:dup,\n\n-- Apply a DES function to generate a prediction\n-- using the copy on the top of the stack. For\n-- a description of the parameters see the DES\n-- reference page.\n10,0.1,0.02,:des,\n\n-- Used to set a threshold. The prediction should\n-- be roughly equal to the line. In this case the\n-- threshold would be 85% of the prediction.\n0.85,:mul,\n\n-- Before              After\n-- 4.                  4. actual\n-- 3.                  3. prediction\n-- 2. actual           2. actual\n-- 1. prediction       1. prediction\n:2over,\n\n-- Create a boolean signal line that is 1\n-- for datapoints where the actual value is\n-- less than the prediction and 0 where it\n-- is greater than or equal the prediction.\n-- The 1 values are where the alert should\n-- trigger.\n:lt,\n\n-- Apply presentation details.\n:rot,$name,:legend,\n:rot,prediction,:legend,\n:rot,:vspan,60,:alpha,alert+triggered,:legend See the [stack language](Stack Language) page for more information."},{"location":"overview.html#memory-storage","title":"Memory Storage","pageTitle":"Overview","text":"Storage for Atlas has been a bit of a sore point. We have tried many backends and ended up moving more and more to a model where pretty much all data is stored in memory either in or off the java heap."},{"location":"overview.html#speed","title":"Speed","pageTitle":"Overview","text":"The primary goal for Atlas is to support queries over dimensional time series data so we can slice and dice to drill down into problems. This means we frequently have a need to perform a large aggregations that involve many data points even though the final result set might be small. As an example consider a simple graph showing the number of requests per second hitting a service for the last 3 hours. Assuming minute resolution that is 180 datapoints for the final output. On a typical service we would get one time series per node showing the number of requests so if we have 100 nodes the intermediate result set is around 18k datapoints. For one service users went hog wild with dimensions breaking down requests by device (~1000s) and country (~50) leading to about 50k time series per node. If we still assume 100 nodes that is about 900M datapoints for the same 3h line. Though obviously we have to be mindful about the explosion of dimensions, we also want that where possible to be a decision based on cost and business value rather than a technical limitation."},{"location":"overview.html#resilience","title":"Resilience","pageTitle":"Overview","text":"What all has to be working in order for the monitoring system to work? If it falls over what is involved in getting it back up? Our focus is primarily operational insight so the top priority is to be able to determine what is going on right now. This leads to the following rules of thumb: Data becomes exponentially less important as it gets older Restoring service is more important than preventing data loss Try to degrade gracefully As a result the internal Atlas deployment breaks up the data into multiple windows based on the window of data they contain.  With this setup we can show the last 6h of data as long as clients can successfully publish. The data is all in memory sharded across machines in the 6h clusters. Because the data and index are all in memory on the local node each instance is self contained and doesn’t need any external service to function. We typically run multiple mirrors of the 6h cluster so data is replicated and we can handle loss of an instance. In AWS we run each mirror in a different zone so that a zone failure will only impact a single mirror. The publish cluster needs to know all of the instance in the mirror cluster and takes care of splitting the traffic up so it goes to the correct shard. The set of mirror instances and shards are assigned based on slots from the Edda autoScalingGroups API. Since the set of instances for the mirrors change rarely, the publish instances can cache the Edda response and still retain successfully publish most data if Edda fails. If an instance is replaced and we can’t update data we would have partial loss for a single shard if the same shard was missing in another mirror. Historical data can also fail in which case graphs would not be able to show data for some older windows. This doesn’t have to be fully continuous, for example a common use case for us is to look at week-over-week (WoW) charts even though the span of the chart might only be a few hours. If the < 4d cluster fails but the < 16d cluster is functioning we could still serve that graph even though we couldn’t show a continuous graph for the full week. A graph would still be shown but would be missing data in the middle. After data is written to the mirrors, they will flush to a persistence layer that is responsible for writing the data to the long term storage in S3. The data at full resolution is kept in S3 and we use hadoop (Elastic MapReduce) for processing the data to perform corrective merging of data from the mirrors, generate reports, and perform rollups into a form that can be loaded into the historical clusters."},{"location":"overview.html#cost","title":"Cost","pageTitle":"Overview","text":"Keeping all data in memory is expensive in-particular with the large growth rate of data. The combination of dimensionality and time based partitioning used for resilience also give us a way to help manage costs. The first way is in controlling the number of replicas. In most cases we are using replicas for redundancy not to provide additional query capacity. For historical data that can be reloaded from stable storage we typically run only one replica as the duration of partial downtime was not deemed to be worth the cost for an additional replica. The second way is as part of the hadoop processing we can compute rollups so that we have a much smaller data volume to load in historical clusters. At Netflix the typical policy is roughly: Cluster Policy < 6h Keeps all data received < 4d ago Keeps most data, we do early rollup by dropping the node dimension on some business metrics < 16d ago Rollup by dropping the node dimension on all metrics older Explicit whitelist, typically recommend BI systems for these use-cases Using these policies we get greatly reduced index sizes for the number of distinct time series despite a significant amount of churn. With auto-scaling and red/black deployment models the set of instances change frequently so typically the intersection of distinct time series from one day to the next is less than 50%. Rollups target the dimensions which lead to that churn giving us much smaller index sizes. Also, in many cases dimensions like node that lead to this increase become less relevant after the node goes away. Deep-dive or investigative use-cases can still access the data using hadoop if needed. Snapshot of index sizes for one region in our environment: < 6h < 4d < 16d"},{"location":"getting-started.html#getting-started","title":"Getting Started","pageTitle":"Getting Started","text":"The instructions on this page are for quickly getting a sample backend server running on a local machine. For other common tasks see: Querying Data: Examples Tutorial Instrumenting Code"},{"location":"getting-started.html#run-a-demo-instance","title":"Run a Demo Instance","pageTitle":"Getting Started","text":"Prerequisites These instructions assume a unix based machine with curl. Other systems may work, but have not been tried. Java 8 or higher is required. To quickly run a version with some synthetic sample data: $ curl -LO https://github.com/Netflix/atlas/releases/download/v1.5.3/atlas-1.5.3-standalone.jar\n$ java -jar atlas-1.5.3-standalone.jar"},{"location":"getting-started.html#explore-available-tags","title":"Explore Available Tags","pageTitle":"Getting Started","text":"The tags API is used to explore available tags and the relationships between them. # show all tags\n$ curl -s 'http://localhost:7101/api/v1/tags'\n\n# show all values of the name, nf.app and type tags\n$ curl -s 'http://localhost:7101/api/v1/tags/name'\n$ curl -s 'http://localhost:7101/api/v1/tags/nf.app'\n$ curl -s 'http://localhost:7101/api/v1/tags/type'\n\n# show all name tags that also have the type tag\n$ curl -s 'http://localhost:7101/api/v1/tags/name?q=type,:has'\n\n# show all name tags that have an nf.app tag with a value of nccp\n$ curl -s 'http://localhost:7101/api/v1/tags/name?q=nf.app,nccp,:eq'"},{"location":"getting-started.html#generate-graphs","title":"Generate Graphs","pageTitle":"Getting Started","text":"These graph API URLs show off a couple of the capabilities of the Atlas backend. See the Examples page for more detailed use cases. # graph all metrics with a name tag value of ssCpuUser, using an :avg aggregation\n$ curl -Lo graph.png 'http://localhost:7101/api/v1/graph?q=name,ssCpuUser,:eq,:avg'\n\n# duplicate the ssCpuUser signal, check if it is greater than 22.8 and display the result as a vertical span with 30% alpha\n$ curl -Lo graph.png 'http://localhost:7101/api/v1/graph?q=name,ssCpuUser,:eq,:avg,:dup,22.8,:gt,:vspan,30,:alpha'"},{"location":"concepts.html#concepts","title":"Concepts","pageTitle":"Concepts","text":"This page is meant to provide a basic overview of some of the key concepts and terminology that is used for Atlas. If you have experience with other monitoring systems, then much of this should be familiar though we might be using different terms."},{"location":"concepts.html#time-series","title":"Time Series","pageTitle":"Concepts","text":"Atlas is a backend for storing and querying dimensional time series data. A time series is a sequence of data points reported at a consistent interval over time. The time interval between successive data points is called the step size. In Atlas each time series is paired with metadata called tags that allow us to query and group the data."},{"location":"concepts.html#tags","title":"Tags","pageTitle":"Concepts","text":"A set of key value pairs associated with a time series. Each time series must have at least one tag with a key of name. To make it more concrete, here is an example of a tag set represented as a JSON object: {\n  \"name\":       \"server.requestCount\",\n  \"status\":     \"200\",\n  \"endpoint\":   \"api\",\n  \"nf.app\":     \"fooserver\",\n  \"nf.cluster\": \"fooserver-main\",\n  \"nf.stack\":   \"main\",\n  \"nf.region\":  \"us-east-1\",\n  \"nf.zone\":    \"us-east-1c\",\n  \"nf.node\":    \"i-12345678\"\n} Usage of tags will typically fall into two categories: Namespace: these are tags necessary to qualify a name so it can be meaningfully aggregated. For example, using the sample above consider computing the sum of all metrics for application fooserver. That number would be meaningless. Properly modelled data should try to make the aggregates meaningful by selecting the name. The sum of all metrics with name = server.requestCount is the overall request count for the service. Dimensions: these are tags used to drill down into the data, for example, see the number of requests with status = 200 or for a particular node. Most tags should fall into this category. When creating metrics it is important to carefully think about how the data should be tagged. See the naming conventions docs for more information."},{"location":"concepts.html#metric","title":"Metric","pageTitle":"Concepts","text":"A metric is a specific quantity being measured, e.g., the number of requests received by a server. In casual language about Atlas metric is often used interchangeably with time series. A time series is one way to track a metric and is the method supported by Atlas. In most cases there will be many time series for a given metric. Going back to the example, request count would usually be tagged with additional dimensions such as status and node. There is one time series for each distinct combination of tags, but conceptually it is the same metric."},{"location":"concepts.html#data-point","title":"Data Point","pageTitle":"Concepts","text":"A data point is a triple consisting of tags, timestamp, and a value. It is important to understand at a high level how data points correlate with the measurement. Consider requests hitting a server, this would typically be measured using a counter. Each time a request is received the counter is incremented. There is not one data point per increment, a data point represents the behavior over a span of time called the step size. The client library will sample the counter once for each interval and report a single value. Suppose that each circle in the diagram below represents a request: 1:00       1:01       1:02       1:03\n ├─●────●●●─┼──────────┼──●───────┤ There are 5 requests shown, 4 from 1:00 to 1:01, and 1 from 1:02 to 1:03. Assuming all requests incremented the same time series, i.e. all other dimensions such as status code are the same, then this would result in three data points. For counters values are always a rate per second, so for a one minute step size the total number of requests would be divided by 60 seconds. So the values stored would be: Time Value 1:01 4 / 60 = 0.0667 1:02 0 / 60 = 0.0000 1:03 1 / 60 = 0.0167"},{"location":"concepts.html#step-size","title":"Step Size","pageTitle":"Concepts","text":"The amount of time between two successive data points in a time series. For Atlas the datapoints will always be on even boundaries of the step size. If data is not reported on step boundaries, it will get normalized to the boundary."},{"location":"concepts.html#normalization","title":"Normalization","pageTitle":"Concepts","text":"In Atlas this usually refers to normalizing data points to step boundaries. Suppose that values are actually getting reported at 30 seconds after the minute instead of exactly on the minute. The values will get normalized to the minute boundary so that all time series in the system are consistent. How a normalized value is computed depends on the data source type. Atlas supports three types indicated by the value of the atlas.dstype tag. In general, you should not need to worry about that, client libraries like spectator will automatically handle tagging based on the data source type. It is recommended to at least skim through the normalization for gauges and rates to better understand how the values you see actually relate to measured data."},{"location":"concepts.html#gauge","title":"Gauge","pageTitle":"Concepts","text":"A value that is sampled from some source and the value is used as is. The last value received will be the value used for the interval. For example: ┌─────────┐                                    ┌─────────┐\n                │    8    │                                    │    8    │\n                │         ├───────                             │         │\n                │         │    6                               │         │\n──────┐         │         │                ┌─────────┐         │         │\n 4    │         │         │                │    4    │         │         │\n      ├─────────┤         │           to   │         ├─────────┤         │\n      │    2    │         │                │         │    2    │         │\n ├─────────┼─────────┼─────────┤           ├─────────┼─────────┼─────────┤\n1:00      1:01      1:02      1:03        1:00      1:01      1:02      1:03"},{"location":"concepts.html#rate","title":"Rate","pageTitle":"Concepts","text":"A rate is a value representing the rate per second since the last reported value. Rate values are normalized using a weighted average. For example: ┌─────────┐\n                │    8    │                                    ┌─────────┐\n                │         ├───────                             │    7    │\n                │         │    6                     ┌─────────┤         │\n──────┐         │         │                          │    5    │         │\n 4    │         │         │                ┌─────────┤         │         │\n      ├─────────┤         │           to   │    3    │         │         │\n      │    2    │         │                │         │         │         │\n ├─────────┼─────────┼─────────┤           ├─────────┼─────────┼─────────┤\n1:00      1:01      1:02      1:03        1:00      1:01      1:02      1:03 In this example, the data is reported at exactly 30s after the minute boundary. So each value represents the average rate per second for 50% of the minute. Time Value 1:01 4 * 0.5 + 2 * 0.5 = 2 + 1 = 3 1:02 2 * 0.5 + 8 * 0.5 = 1 + 4 = 5 1:03 8 * 0.5 + 6 * 0.5 = 4 + 3 = 7 If many samples are received for a given interval, then they will each be weighted based on the fraction of the interval they represent. When no previous sample exists, value will be treated as the average rate per second over the previous step. This behavior is important to avoid under counting the contribution from a previous interval. The example below shows what happens if there is no previous or next sample: ┌─────────┐\n                │    8    │\n                │         │\n                │         │                          ┌─────────┐\n                │         │                          │    5    ├─────────┐\n                │         │                          │         │    4    │\n      ┌─────────┤         │           to        1    │         │         │\n      │    2    │         │                ┌─────────┤         │         │\n ├─────────┼─────────┼─────────┤           ├─────────┼─────────┼─────────┤\n1:00      1:01      1:02      1:03        1:00      1:01      1:02      1:03 Why perform weighted averaging for rates instead of the simpler last value approach used with gauges? Because it gives us a better summary of what we actually know from the measurements received. In practical terms: It avoids dropping information if samples are more frequent than the step. Suppose we have a 1 minute step, but data is actually betting reported every 10s. For this example, assume we get 1, 5, 90, 5, 4, and 2. The last value normalization used with gauges would end up with a value of 2. The rate normalization will give 17.833. Each value is a rate per second, so if you take the (1 + 5 + 90 + 5 + 4 + 2) * 10 = 1070 actual events measured during the interval. That is equivalent to 17.833 * 60 indicating we have an accurate average rate for the step size. Avoids skewing the data causing misleading spikes or drops in the aggregates. Using Atlas you will typically be looking at an aggregate of time series rather than an individual time series that was reported. With last value it can have the effect of skewing samples to a later interval. Suppose the client is reporting once a minute at 5s after the minute. That value indicates more about the previous interval than it does the current one. During traffic transitions, such as moving traffic over to a new cluster or even some auto-scaling events, differences in this skew can result in the appearance of a drop because there will be many new time series getting reported with a delayed start. For existing time series it is still skewed, but tends to be less noticeable. The weighted averaging avoids these problems for the most part."},{"location":"concepts.html#counter","title":"Counter","pageTitle":"Concepts","text":"Counter is similar to rate, except that the value reported is monotonically increasing and will be converted to a rate by the backend. The conversion is done by computing the delta between the current sample and the previous sample and dividing by the time between the samples. After that it is the same as a rate. Note, that unless the input is a montonically increasing counter it is generally better to have the client perform rate conversion. Since, the starting value is unknown, at least two samples must be received before the first delta can be computed. This means that new time series relying on counter type will be delayed by one interval."},{"location":"concepts.html#aggregation-function","title":"Aggregation Function","pageTitle":"Concepts","text":"Combines a set of time series into a single time series. A conceptual measurement such as cpu usage or requests per second. A metric will usually end up getting reported as a set of time series, for example a cpu usage time series for each node."},{"location":"concepts.html#consolidation-function","title":"Consolidation Function","pageTitle":"Concepts","text":"Converts a time series from a primary step size to a larger step size that is a multiple of the primary. For example, if you have a time series with data points each minute and you want a time series with data points every 5 minutes. In most cases you won’t use a consolidation function directly. The system will automatically apply consolidation to ensure there is at least one pixel per datapoint on the rendered images."},{"location":"concepts.html#nan-aware-operations","title":"NaN Aware Operations","pageTitle":"Concepts","text":"This typically refers to how binary operations treat NaN values in the data. A NaN value means that the value is unknown for a given interval. NaN aware operations will keep the value as NaN if both sides are NaN, but treat it as a special value such as 0.0 otherwise. This is useful to avoid gaps in the graph if a subset of time series used in an aggregate have gaps in data. lhs rhs result NaN NaN op(NaN, NaN) v1 NaN op(v1, 0.0) NaN v2 op(0.0, v2) v1 v2 op(v1, v2)"},{"location":"presentations.html#presentations","title":"Presentations","pageTitle":"Presentations","text":""},{"location":"presentations.html#atlas-overview","title":"Atlas Overview","pageTitle":"Presentations","text":"High level overview of Atlas backend. Presenter @brharrington Date 2014-12-16 Links PDF"},{"location":"presentations.html#atlas-telemetry-a-platform-begets-an-ecosystem","title":"Atlas Telemetry: A Platform Begets an Ecosystem","pageTitle":"Presentations","text":"Summary of Atlas and surrounding tooling used at Netflix. Presenter @copperlight Date 2015-11-17 Links Video"},{"location":"presentations.html#canary-analyze-all-the-things","title":"Canary Analyze All The Things","pageTitle":"Presentations","text":"Overview of canary analysis at Netflix. Canary analysis is one type of tooling that relies heavily on Atlas data internally. Presenter @royrapoport Date 2014-08-28 Links Video and Slides"},{"location":"presentations.html#cloud-operations-at-netflix","title":"Cloud Operations at Netflix","pageTitle":"Presentations","text":"Summary of how Netflix thinks about operations. Not specific to Atlas, but Atlas is one of the tools involved. Presenter @royrapoport Date 2013-11-20 Links Video"},{"location":"presentations.html#monitoring-monitoring-systems-at-netflix","title":"Monitoring Monitoring Systems at Netflix","pageTitle":"Presentations","text":"Monitorama 2017 presentation on how we monitor the health of our monitoring systems. Presenter @royrapoport Date 2017-05-22 Links Video"},{"location":"presentations.html#netflix-amazon-s3-and-amazon-elastic-mapreduce-to-monitor-at-gigascale","title":"Netflix: Amazon S3 and Amazon Elastic MapReduce to Monitor at Gigascale","pageTitle":"Presentations","text":"AWS re:Invent presentation on the usage of S3 and EMR for Atlas. Presenter @royrapoport Date 2013-10-26 Links Video"},{"location":"asl/index.html#stack-language","title":"Stack Language","pageTitle":"Stack Language","text":"Atlas Stack Language is designed to be a stable method of representing complex data queries in a URL-friendly format. It is loosely based on the RPN expressions supported by Tobias Oetiker’s rrdtool. The following is an example of a stack language expression: nf.cluster,discovery,:eq,(,nf.zone,),:by This example pushes two strings nf.cluster and discovery onto the stack and then executes the command :eq. The equal command pops two strings from the stack and pushes a query object onto the stack. The behavior can be described by the stack effect String:key String:value – Query. We then push a list of tag keys to the stack and execute the command :by to group the results."},{"location":"asl/index.html#parts","title":"Parts","pageTitle":"Stack Language","text":"There are only four reserved symbols used for structuring the expression: ,:() Commas separate items on the stack. So a,b puts two strings on the stack with values \"a\" and \"b\". Colon is used to prefix operations. If the first character is a colon the item will be treated as a command to run. For example, a,:dup, will push \"a\" on the stack and then execute the duplicate operation. Parenthesis are used to indicate the start and end of a list. The expression (,) puts an empty list on the stack. Commands inside of a list will not be executed unless the list is passed to the call command. For example, (,:dup,) will push a list with a single string value of \":dup\" on to the stack."},{"location":"asl/index.html#data-model","title":"Data Model","pageTitle":"Stack Language","text":"The stack language is primarily used for representing expressions over tagged time series data. A tag is a string key value pair used to describe a measurement. Atlas requires at least one tag with a key of name. Example tags represented as a JSON map: {\n  \"name\":       \"jvm.gc.pause\",\n  \"cause\":      \"Allocation_Failure\",\n  \"statistic\":  \"count\",\n  \"nf.app\":     \"www\",\n  \"nf.cluster\": \"www-main\",\n  \"nf.asg\":     \"www-main-v001\",\n  \"nf.stack\":   \"main\",\n  \"nf.node\":    \"i-01\",\n  \"nf.region\":  \"us-east-1\",\n  \"nf.zone\":    \"us-east-1a\"\n} Typically tags should be dimensions that allow you to use the name as a pivot and other tags to drill down into the data. The tag keys are similar to columns in a traditional table, however, it is important to note that not all time series will have the same set of tag keys. The tags are used to identify a time series, which conceptually is a set of timestamp value pairs. Here is a simplified data set shown as a table: name app node values cpuUsage www i-01 [(05:00, 33.0), (05:01, 31.0)] cpuUsage www i-02 [(05:00, 20.0), (05:01, 37.0)] cpuUsage db i-03 [(05:00, 57.0), (05:01, 62.0)] diskUsage www i-01 [(05:00, 9.0), (05:01, 9.0)] diskUsage www i-02 [(05:00, 7.0), (05:01, 8.0)] requestRate www [(05:00, 33.0), (05:01, 31.0)] The table above will be used for the examples in later sections."},{"location":"asl/index.html#simple-expressions","title":"Simple Expressions","pageTitle":"Stack Language","text":"All expressions generally have four parts: Query: selects a set of time series. Aggregation: defines how to combine the selected time series. Math: manipulate the time series values or combine aggregated results with binary operations. Presentation: adjust how the data is presented in a chart."},{"location":"asl/index.html#query","title":"Query","pageTitle":"Stack Language","text":"The query is used to select a set of time series. The primary query operators are :eq and :and. For a full list see the query section of the reference page. Sample query to select all time series where the key node is equal to i-01: node,i-01,:eq If you are familiar with SQL and assume that tag keys are column names, then this would be equivalent to: select * from time_series where node = 'i-01'; Using the example data set this query would return the following subset: name app node values cpuUsage www i-01 [(05:00, 33.0), (05:01, 31.0)] diskUsage www i-01 [(05:00, 9.0), (05:01, 9.0)] To get just the cpu usage for that node, use :and: node,i-01,:eq,name,cpuUsage,:eq,:and This would result in: name app node values cpuUsage www i-01 [(05:00, 33.0), (05:01, 31.0)]"},{"location":"asl/index.html#aggregation","title":"Aggregation","pageTitle":"Stack Language","text":"An aggregation function maps a set of time series that matched the query to a single time series. Atlas supports four aggregate functions: sum, min, max, and count. If no aggregate is specified on an expression, then sum will be used implicitly. Using the example data set, these two expressions would be equivalent: app,www,:eq,name,cpuUsage,:eq,:and\napp,www,:eq,name,cpuUsage,:eq,:and,:sum And would result in a single output time series: name app values cpuUsage www [(05:00, 53.0), (05:01, 68.0)] Note that the node is not present in the output. The set of tags on the output will be ones with exact matches in the query clause or explicitly listed in the group by. If you wanted the max cpu for the application, then you would write: app,www,:eq,name,cpuUsage,:eq,:and,:max What if we want the average? The count aggregate is used to determine how many time series had a value for a given time. To get the average we divide the sum by the count. app,www,:eq,name,cpuUsage,:eq,:and,\n:dup,\n:sum,\n:swap,\n:count,\n:div There is a helper macro :avg that will do this for you, so you can write: app,www,:eq,name,cpuUsage,:eq,:and,:avg"},{"location":"asl/index.html#group-by","title":"Group By","pageTitle":"Stack Language","text":"In many cases we want to group the results that were selected and return one aggregate per group. As an example suppose I want to see maximum cpu usage by application: name,cpuUsage,:eq,:max,(,app,),:by Using the example data set, this would result in a two output time series: name app values cpuUsage www [(05:00, 33.0), (05:01, 37.0)] cpuUsage db [(05:00, 57.0), (05:01, 62.0)]"},{"location":"asl/index.html#math","title":"Math","pageTitle":"Stack Language","text":"Once you have a set of lines, it can be useful to manipulate them. The supported operations generally fall into two categories: unary operations to alter a single time series and binary operations that combine two time series. Examples of unary operations are negate and absolute value. To apply the absolute value: app,web,:eq,name,cpu,:eq,:and,:sum,:abs Multiple operations can be applied, for example, negating the line then applying the absolute value: app,web,:eq,name,cpu,:eq,:and,:sum,:neg,:abs Common binary operations are add, subtract, multiply, and divide. The aggregation section has an example of using divide to compute the average. For a complete list see the math section of the reference page."},{"location":"asl/alerting.html#alerting-expressions","title":"Alerting Expressions","pageTitle":"Alerting Expressions","text":"The stack language provides some basic techniques to convert an input line into a set of signals that can be used to trigger and visualize alert conditions. This section assumes a familiarity with the stack language and the alerting philosophy"},{"location":"asl/alerting.html#threshold-alerts","title":"Threshold Alerts","pageTitle":"Alerting Expressions","text":"To start we need an input metric. For this example the input will be a sample metric showing high CPU usage for a period: /api/v1/graph?\n  e=2012-01-01T07:00\n  &h=100\n  &l=0\n  &q=\n    nf.app,alerttest,:eq,\n    name,ssCpuUser,:eq,\n    :and,\n    :sum\n  &s=e-3h\n  &tz=UTC  Lets say we want to trigger an alert when the CPU usage goes above 80%. To do that simply use the :gt operator and append 80,:gt to the query: /api/v1/graph?\n  e=2012-01-01T07:00\n  &h=100\n  &l=0\n  &q=\n    nf.app,alerttest,:eq,\n    name,ssCpuUser,:eq,\n    :and,\n    :sum,\n    80,:gt\n  &s=e-3h\n  &tz=UTC  The result is a signal line that is non-zero, typically 1, when in a triggering state and zero when everything is fine."},{"location":"asl/alerting.html#dampening","title":"Dampening","pageTitle":"Alerting Expressions","text":"Our threshold alert above will trigger if the CPU usage is ever recorded to be above the threshold. Alert conditions are often combined with a check for the number of occurences. This is done by using the :rolling-count operator to get a line showing how many times the input signal has been true withing a specified window and then applying a second threshold to the rolling count. Input Rolling-count Dampened signal nf.app,alerttest,:eq,\nname,ssCpuUser,:eq,\n:and,\n:sum,\n80,:gt nf.app,alerttest,:eq,\nname,ssCpuUser,:eq,\n:and,\n:sum,\n80,:gt,\n5,:rolling-count nf.app,alerttest,:eq,\nname,ssCpuUser,:eq,\n:and,\n:sum,\n80,:gt,\n5,:rolling-count,\n4,:gt"},{"location":"asl/alerting.html#visualization","title":"Visualization","pageTitle":"Alerting Expressions","text":"A signal line is useful to tell whether or not something is in a triggered state, but can be difficult for a person to follow. Alert expressions can be visualized by showing the input, threshold, and triggering state on the same graph. /api/v1/graph?\n  e=2012-01-01T07:00\n  &h=100\n  &l=0\n  &q=\n    nf.app,alerttest,:eq,\n    name,ssCpuUser,:eq,\n    :and,\n    :sum,\n    80,:2over,\n    :gt,\n    :vspan,\n    40,:alpha,\n    triggered,:legend,\n    :rot,\n    input,:legend,\n    :rot,\n    threshold,:legend,\n    :rot\n  &s=e-3h\n  &tz=UTC"},{"location":"asl/alerting-philosophy.html#alerting-philosophy","title":"Alerting Philosophy","pageTitle":"Alerting Philosophy","text":"It is recommended for all alerts to adhere to the follow guidelines: Keep conditions simple. Alerts should be actionable. Check for measured failure on critical paths rather than a lack of success. Alerts should not have special cases for routine maintenance. Consider how the alert check can fail."},{"location":"asl/alerting-philosophy.html#keep-it-simple","title":"Keep It Simple","pageTitle":"Alerting Philosophy","text":"When an alert triggers, it should be easy to understand why. Similarly, if an alert doesn’t fire, then it should be easy to check and see what happened. The more complicated an alert condition becomes, the harder it is to understand and debug. It is recommended to keep alert rules as a simple expression with a threshold and number of occurrences. An example of this is the following rule: CPU Usage > 80% for at least 5 minutes Multiple signals should only be combined if it improves the effectiveness of the alert. For example, what is an appropriate threshold for the number of requests that have error responses? What happens to that threshold if your cluster auto-scales? It is more effective to define the threshold as a percentage of total requests: (Num Errors / Num Total) > 0.01 for at least 5 minutes In some cases, a low volume can make the percentages less meaningful and result in false positives. For example, if your daily traffic pattern follows a sine curve, then the troughs may not represent a meaningful error percentage. Another example might be during failover exercises, if traffic has been failed over to another cluster. One way to compensate for this is to check the failure rate and overall volume: Percentage of Failures > X AND Volume > Y As a general rule, bias towards simplicity. If you are creating more complex expressions, then stop and think about why that complexity is needed. Are there other signals available that are easier to use? Can the application be changed so that it reports metrics which make it easier to diagnose?"},{"location":"asl/alerting-philosophy.html#actionable-alerts","title":"Actionable Alerts","pageTitle":"Alerting Philosophy","text":"If an alert fires and sends a notification to users, someone should be motivated to investigate the problem. Alerts that are noisy or not actionable train people to ignore or filter out alert notifications. For cases where the response to an alert can be automated, such as terminating a bad instance, it shouldn’t send out a notification unless there is a failure to perform the action. If you want a summary of cluster health, then use dashboards or reporting tools for this function; don’t attempt to do this via alert notifications. Alerts should check something important. To setup effective alerts, you need to understand the application and have ways to detect failures for critical functionality. Avoid general system-type alerts that won’t be investigated. For example, should you alert on high CPU usage? If you have done squeeze testing and you have information to indicate how CPU usage impacts the application, then it can be useful and it will provide a way to know a problem is coming before it impacts clients of the service. If you do not have this knowledge, then your alert may be under-tuned, leading to noisy notifications that may be ignored."},{"location":"asl/alerting-philosophy.html#check-for-measured-failure","title":"Check for Measured Failure","pageTitle":"Alerting Philosophy","text":"It is better to check for failures rather than trying to trigger based on an absence of information or a reduction in the amount of success."},{"location":"asl/alerting-philosophy.html#absence-of-information","title":"Absence of Information","pageTitle":"Alerting Philosophy","text":"A typical example of this is a process that runs over a longer time period. For example, suppose we have an application that updates a metadata cache once per day and it takes an hour to refresh. It is not recommended to send an event on refresh success and then configure alerts based on the absence of the success event. Design the signals so you have a clear way to understand what error conditions may be occurring on and then alert if there is a problem. In this example, a better design would use a gauge that reports the loading time and a gauge that reports the age of the cache. You can then add alerts when the gauges for these error conditions exceed unacceptable thresholds."},{"location":"asl/alerting-philosophy.html#reduction-in-success","title":"Reduction in Success","pageTitle":"Alerting Philosophy","text":"Let’s say we have a server that is taking traffic and we want to know if users are experiencing problems. How should we go about this? It is often tempting to look for things like a drop in the number of successful requests, because this can be a generic catch-all for many types of problems. However, alerts of this sort are inherently noisy. How do you know what the number of requests should be? While there are various schemes for trying to predict the behavior, you will spend a lot of time tuning alerts of this nature to get them to the point where they are not too noisy, but they still catch real issues. Further, these schemes cannot differentiate between problems for the service and unrelated drops such as a client having problems and failing to make the request in the first place. If you’re not going to investigate these alerts when they fire or invest in tuning and maintaining them, just avoid this type of alert altogether. A better approach is to alert on the number of failures you are seeing from a service. Thresholds can often be determined automatically by looking at the percent of all requests that are failures. For middle tier services, it is also likely that data from the clients can be used to see a percentage of failure from a client perspective instead of, or in addition to, the server side view."},{"location":"asl/alerting-philosophy.html#avoid-special-cases","title":"Avoid Special Cases","pageTitle":"Alerting Philosophy","text":"Alerts shouldn’t have to be tuned or suppressed during regular maintenance such as replacing instance or doing deployments. As a simple example, consider an alert on the rate of failures. The general assumption would be that a deployment should not be noticed by clients and therefore the alert is still relevant. Alerts that are actionable and look for measured failure tend to work well. If a new instance is coming up, a lack of activity will mean a lack of failures until traffic is being received. At that time if there are failures they should be noticed."},{"location":"asl/alerting-philosophy.html#startup-behavior","title":"Startup Behavior","pageTitle":"Alerting Philosophy","text":"What about different behavior during startup? Consider some examples for an application that has a long initialization time (~20 minutes) before it can take traffic: Discovery service state during initialization. Healthcheck failures during initialization. Performance may be different while starting. CPU usage is high while initializing but stabilizes and remains low during normal operation. For a discovery service like Eureka, the duration of the startup time shouldn’t be an issue because the state clearly indicates if it is STARTING vs DOWN. If the healthcheck is used for a load balancer, then the decision to send traffic to instances should be fairly sensitive in order to minimize the impact to users. The bigger concern is the number of occurrences of healthcheck failures in a row, which can trigger automated actions like terminating an instance. When evaluating healthcheck failures, there are two distinct conditions to evaluate: non-200 responses and connection timeouts. The healthcheck logic should be tied to the Eureka heartbeat so that if the healthcheck is failing due to a non-200 response, the discovery state will be DOWN after initialization is complete. For the first condition, the alert should check for the number of occurrence of the DOWN state in the discovery service which will not trigger for the STARTING state used during application initialization. For the second condition, you would need to check for a disparity between the published discovery state and the healthcheck state: (DiscoveryStatus is UP) AND (Healthcheck != 200) for N minutes Note, unless you really need to do this it is probably better to just look at the healthcheck and have the num occurrences set to be longer than the typical startup time. For the CPU example, first reconsider whether general system check alerts are actually useful. Is it going to help you catch a real problem and be investigated when it triggers? If not, don’t setup an alert on CPU and rely on alerts that check for failures on the critical path. If it is useful and you have squeeze testing results or other information so you know when a proxy metric like CPU actually indicates a problem, then you can configure it restricted with some signal that indicates the status. However, keep in mind that not all systems will allow complex expressions. For example, if you are auto-scaling will you be able to send the data such that it doesn’t incorrectly skew the alarm? The more signals that are combined the harder it is to understand the alert and the more likely it is to fail in unexpected ways. Before adding more layers of duct tape think hard about the application and if you can change it to be easier to monitor and diagnose."},{"location":"asl/alerting-philosophy.html#deployments","title":"Deployments","pageTitle":"Alerting Philosophy","text":"At Netflix, a common deployment model is red/black. In this model, a new auto-scaling group the same size as the existing one will be created, traffic will transition over, and eventually the old auto-scaling group (ASG) will be deleted. This can create false alarms if you haven’t thought about the signals being used to fire alerts. The most common alerting problem that occurs during deployments is related the use of averages. For example, the average request rate will drop in half if a new ASG comes up and you are aggregating across a cluster consisting of both old and new ASGs. If you follow the advice given earlier about crafting alerts based on a percentage of errors reported by clients of the application, then aggregating across clusters by sum usually won’t be a problem. If the deployment is going well, then the overall failure rate seen by clients shouldn’t be impacted. Another example of a deployment alerting problem is latency measurements. How can you tell the average latency across a cluster composed of new and old ASGs? Rather than trying to special case or exclude the new group of instances, you should define the alert signal based on the actual activity seen. If there is no activity within an ASG, then it will not impact the signal. Metrics libraries like Spectator send both a totalTime and count measurement separately to the backend. This allows the average to be computed using a simple sum aggregate with division: Sum(totalTime per instance in cluster) / Sum(count per instance in cluster) This calculation demonstrates how instances that are not receiving traffic will not contribute anything to the sums."},{"location":"asl/alerting-philosophy.html#think-about-failure","title":"Think About Failure","pageTitle":"Alerting Philosophy","text":"An effective alert needs to be able to fire when there is a problem. However, when problems occur, it is possible that the problem will also impact the underlying data or mechanisms used to detect issues for the alert. It is worthwhile to spend time thinking about the ways in which your alerts can fail to detect events."},{"location":"asl/alerting-philosophy.html#how-can-signals-fail-","title":"How Can Signals Fail?","pageTitle":"Alerting Philosophy","text":"The simplest area to think about is what is collecting and reporting the data. For example, if data is being reported by the plugin running in the application, then it won’t work if the application crashes or cannot start. It is recommended to have some basic alerts using a data pipeline that will fail independently from the application. At Netflix, this typically involves checking the following conditions: The healthcheck is responding with 200. This signal indicates that a remote system was able to connect and query the application healthcheck. So the application is running and inbound traffic made it in. The application is registered with Eureka. Eureka uses a heartbeat mechanism, so checking the registration tells you the application is running and it is able to successfully send the heartbeat request. The metric data for those signals comes from a separate poller application. If these succeed, then the application should be healthy enough that alerts triggered from data local to the instance should be working."},{"location":"asl/des.html#double-exponential-smoothing","title":"Double Exponential Smoothing","pageTitle":"Double Exponential Smoothing","text":"Double exponential smoothing (DES) is a simple technique for generating a smooth trend line from another time series. This technique is often used to generate a dynamic threshold for alerting. Warning Alerts on dynamic thresholds should be expected to be noisy. They are looking for strange behavior rather than an actual problem causing impact. Make sure you will actually spend the time to tune and investigate the alarms before using this approach. See the alerting philosophy guide for more information on best practices."},{"location":"asl/des.html#tuning","title":"Tuning","pageTitle":"Double Exponential Smoothing","text":"The :des operator takes 4 parameters: An input time series training - the number of intervals to use for warming up before generating an output alpha - is a data smoothing factor beta - is a trend smoothing factor"},{"location":"asl/des.html#training","title":"Training","pageTitle":"Double Exponential Smoothing","text":"The training parameter defines how many intervals to allow the DES to warmup. In the graph below the gaps from the start of the chart to the smoothed lines reflects the training window used:  Typically a training window of 10 has been sufficient as DES will adjust to the input fairly quick. However, in some cases if there is a massive change in the input it can cause DES to oscillate, for example:"},{"location":"asl/des.html#alpha","title":"Alpha","pageTitle":"Double Exponential Smoothing","text":"Alpha is the data smoothing factor. A value of 1 means no smoothing. The closer the value gets to 0 the smoother the line should get. Example:"},{"location":"asl/des.html#beta","title":"Beta","pageTitle":"Double Exponential Smoothing","text":"Beta is a trend smoothing factor. Visually it is most apparent when alpha is small. Example with alpha = 0.01:"},{"location":"asl/des.html#recommended-values","title":"Recommended Values","pageTitle":"Double Exponential Smoothing","text":"Experimentally we have converged on 3 sets of values based on how quickly it should adjust to changing levels in the input signal. Helper Alpha Beta :des-fast 0.1 0.02 :des-slower 0.05 0.03 :des-slow 0.03 0.04 Here is an example of how they behave for a sharp drop and recovery:  For a more gradual drop:  If the drop is smooth enough then DES can adjust without ever triggering."},{"location":"asl/des.html#alerting","title":"Alerting","pageTitle":"Double Exponential Smoothing","text":"For alerting purposes the DES line will typically get multiplied by a fraction and then checked to see whether the input line drops below the DES value for a given interval. -- Query to generate the input line\nnf.cluster,alerttest,:eq,\nname,requestsPerSecond,:eq,:and,\n:sum,\n\n-- Create a copy on the stack\n:dup,\n\n-- Apply a DES function to generate a prediction\n:des-fast,\n\n-- Used to set a threshold. The prediction should\n-- be roughly equal to the line. In this case the\n-- threshold would be 85% of the prediction.\n0.85,:mul,\n\n-- Create a boolean signal line that is 1\n-- for datapoints where the actual value is\n-- less than the prediction and 0 where it\n-- is greater than or equal the prediction.\n-- The 1 values are where the alert should\n-- trigger.\n:lt,\n\n-- Apply presentation details.\n:rot,$name,:legend, The vertical spans show when the expression would have triggered with due to the input dropping below the DES line at 85%:"},{"location":"asl-reference/index.html#stack-language-reference","title":"Stack Language Reference","pageTitle":"Stack Language Reference","text":""},{"location":"asl-reference/index.html#stack","title":"Stack","pageTitle":"Stack Language Reference","text":""},{"location":"asl-reference/index.html#query","title":"Query","pageTitle":"Stack Language Reference","text":"Query operations are used to select the set of time series based on tags. :eq :and"},{"location":"asl-reference/2over.html#2over","title":"2over","pageTitle":"2over","text":""},{"location":"asl-reference/2over.html#signature","title":"Signature","pageTitle":"2over","text":"a b -- a b a b"},{"location":"asl-reference/2over.html#summary","title":"Summary","pageTitle":"2over","text":"Duplicates the top two items on the stack. This operation is equivalent to :over,:over. See the over operator for more information."},{"location":"asl-reference/-rot.html#rot","title":"-rot","pageTitle":"-rot","text":""},{"location":"asl-reference/-rot.html#signature","title":"Signature","pageTitle":"-rot","text":"* a b -- b * a"},{"location":"asl-reference/-rot.html#summary","title":"Summary","pageTitle":"-rot","text":"Rotate the stack so that the item at the top is now at the bottom. To rotate in the reverse direction use the rot operator."},{"location":"asl-reference/and.html#and","title":"and","pageTitle":"and","text":"There are two overloaded variants of the :and operator: Query variant that is used to help restrict the set of time series used for an expression. Math variant that is used to combine to signal time series."},{"location":"asl-reference/and.html#query-variant","title":"Query Variant","pageTitle":"and","text":""},{"location":"asl-reference/and.html#signature","title":"Signature","pageTitle":"and","text":"Query Query -- Query"},{"location":"asl-reference/and.html#summary","title":"Summary","pageTitle":"and","text":"Query expression that matches if both sub queries match. Suppose you have three time series: name=http.requests, status=200, nf.app=server name=sys.cpu, type=user, nf.app=foo name=sys.cpu, type=user, nf.app=bar The query name,sys.cpu,:eq,nf.app,foo,:eq,:and would match: name=sys.cpu, type=user, nf.app=foo"},{"location":"asl-reference/and.html#math-variant","title":"Math Variant","pageTitle":"and","text":""},{"location":"asl-reference/and.html#signature","title":"Signature","pageTitle":"and","text":"TimeSeriesExpr TimeSeriesExpr -- TimeSeriesExpr"},{"location":"asl-reference/call.html#call","title":"call","pageTitle":"call","text":""},{"location":"asl-reference/call.html#signature","title":"Signature","pageTitle":"call","text":"? List -- ?"},{"location":"asl-reference/clear.html#clear","title":"clear","pageTitle":"clear","text":""},{"location":"asl-reference/clear.html#signature","title":"Signature","pageTitle":"clear","text":"* -- <empty>"},{"location":"asl-reference/cq.html#cq","title":"cq","pageTitle":"cq","text":""},{"location":"asl-reference/cq.html#signature","title":"Signature","pageTitle":"cq","text":"Expr Query -- Expr"},{"location":"asl-reference/des.html#des","title":"des","pageTitle":"des","text":""},{"location":"asl-reference/des.html#signature","title":"Signature","pageTitle":"des","text":"TimeSeriesExpr training:Int alpha:Double beta:Double -- TimeSeriesExpr"},{"location":"asl-reference/des.html#summary","title":"Summary","pageTitle":"des","text":"Double exponential smoothing. For most use-cases sliding DES should be used instead to ensure a deterministic prediction."},{"location":"asl-reference/des-epic-signal.html#des-epic-signal","title":"des-epic-signal","pageTitle":"des-epic-signal","text":""},{"location":"asl-reference/des-epic-signal.html#signature","title":"Signature","pageTitle":"des-epic-signal","text":"TimeSeriesExpr\ntraining:Int\nalpha:Double\nbeta:Double\nmaxPercent:Double\nminPercent:Double\nnoise:Double         -- TimeSeriesExpr"},{"location":"asl-reference/des-epic-viz.html#des-epic-viz","title":"des-epic-viz","pageTitle":"des-epic-viz","text":""},{"location":"asl-reference/des-epic-viz.html#signature","title":"Signature","pageTitle":"des-epic-viz","text":"TimeSeriesExpr\ntraining:Int\nalpha:Double\nbeta:Double\nmaxPercent:Double\nminPercent:Double\nnoise:Double         -- TimeSeriesExpr"},{"location":"asl-reference/des-fast.html#des-fast","title":"des-fast","pageTitle":"des-fast","text":""},{"location":"asl-reference/des-fast.html#signature","title":"Signature","pageTitle":"des-fast","text":"TimeSeriesExpr -- TimeSeriesExpr"},{"location":"asl-reference/des-fast.html#summary","title":"Summary","pageTitle":"des-fast","text":"Helper for computing DES using settings to quickly adjust to the input line. See recommended values for more information. For most use-cases the sliding DES variant :sdes-fast should be used instead."},{"location":"asl-reference/des-slow.html#des-slow","title":"des-slow","pageTitle":"des-slow","text":""},{"location":"asl-reference/des-slow.html#signature","title":"Signature","pageTitle":"des-slow","text":"TimeSeriesExpr -- TimeSeriesExpr"},{"location":"asl-reference/des-slow.html#summary","title":"Summary","pageTitle":"des-slow","text":"Helper for computing DES using settings to quickly adjust to the input line. See recommended values for more information. For most use-cases the sliding DES variant :sdes-slow should be used instead."},{"location":"asl-reference/des-slower.html#des-slower","title":"des-slower","pageTitle":"des-slower","text":""},{"location":"asl-reference/des-slower.html#signature","title":"Signature","pageTitle":"des-slower","text":"TimeSeriesExpr -- TimeSeriesExpr"},{"location":"asl-reference/des-slower.html#summary","title":"Summary","pageTitle":"des-slower","text":"Helper for computing DES using settings to quickly adjust to the input line. See recommended values for more information. For most use-cases the sliding DES variant :sdes-slower should be used instead."},{"location":"asl-reference/dist-avg.html#dist-avg","title":"dist-avg","pageTitle":"dist-avg","text":""},{"location":"asl-reference/dist-avg.html#signature","title":"Signature","pageTitle":"dist-avg","text":"Query -- TimeSeriesExpr"},{"location":"asl-reference/dist-avg.html#summary","title":"Summary","pageTitle":"dist-avg","text":"Compute standard deviation for timers and distribution summaries."},{"location":"asl-reference/drop.html#drop","title":"drop","pageTitle":"drop","text":""},{"location":"asl-reference/drop.html#signature","title":"Signature","pageTitle":"drop","text":"* a -- *"},{"location":"asl-reference/drop.html#summary","title":"Summary","pageTitle":"drop","text":"Remove the item on top of the stack. To remove all items see the clear operation."},{"location":"asl-reference/dup.html#dup","title":"dup","pageTitle":"dup","text":""},{"location":"asl-reference/dup.html#signature","title":"Signature","pageTitle":"dup","text":"a -- a a"},{"location":"asl-reference/dup.html#summary","title":"Summary","pageTitle":"dup","text":"Duplicates the item on the top of the stack. See also: over and pick."},{"location":"asl-reference/each.html#each","title":"each","pageTitle":"each","text":""},{"location":"asl-reference/each.html#signature","title":"Signature","pageTitle":"each","text":"items:List f:List -- f(items[0]) ... f(items[N])"},{"location":"asl-reference/each.html#summary","title":"Summary","pageTitle":"each","text":"For each item in the input list, push it on the stack and apply a function."},{"location":"asl-reference/eq.html#eq","title":"eq","pageTitle":"eq","text":""},{"location":"asl-reference/eq.html#signature","title":"Signature","pageTitle":"eq","text":"k:String v:String -- Query"},{"location":"asl-reference/fcall.html#fcall","title":"fcall","pageTitle":"fcall","text":""},{"location":"asl-reference/fcall.html#signature","title":"Signature","pageTitle":"fcall","text":"? name:String -- ?"},{"location":"asl-reference/fcall.html#summary","title":"Summary","pageTitle":"fcall","text":"Invoke a function with the provided name. The function must have previously been stored with that name using the set operation. Equivalent to running: ,:get,:call See the call operation for more information."},{"location":"asl-reference/format.html#format","title":"format","pageTitle":"format","text":""},{"location":"asl-reference/format.html#signature","title":"Signature","pageTitle":"format","text":"pattern:String args:List -- str:String"},{"location":"asl-reference/format.html#summary","title":"Summary","pageTitle":"format","text":"Format a string using a printf style pattern"},{"location":"asl-reference/freeze.html#freeze","title":"freeze","pageTitle":"freeze","text":""},{"location":"asl-reference/freeze.html#signature","title":"Signature","pageTitle":"freeze","text":"* -- <empty>"},{"location":"asl-reference/get.html#get","title":"get","pageTitle":"get","text":""},{"location":"asl-reference/get.html#signature","title":"Signature","pageTitle":"get","text":"k -- v"},{"location":"asl-reference/get.html#summary","title":"Summary","pageTitle":"get","text":"Pop the key and push the value associated with that key on top of the stack. The key must have been previously associated with a value using the set operation."},{"location":"asl-reference/list.html#list","title":"list","pageTitle":"list","text":""},{"location":"asl-reference/list.html#signature","title":"Signature","pageTitle":"list","text":"* -- List(*)"},{"location":"asl-reference/list.html#summary","title":"Summary","pageTitle":"list","text":"Pop all items off the stack and push them as a list. See also: nlist."},{"location":"asl-reference/map.html#map","title":"map","pageTitle":"map","text":""},{"location":"asl-reference/map.html#signature","title":"Signature","pageTitle":"map","text":"items:List f:List -- List(f(items[0]) ... f(items[N]))"},{"location":"asl-reference/map.html#summary","title":"Summary","pageTitle":"map","text":"Create a new list by applying a function to all elements of a list."},{"location":"asl-reference/ndrop.html#ndrop","title":"ndrop","pageTitle":"ndrop","text":""},{"location":"asl-reference/ndrop.html#signature","title":"Signature","pageTitle":"ndrop","text":"aN ... a0 N -- aN"},{"location":"asl-reference/ndrop.html#summary","title":"Summary","pageTitle":"ndrop","text":"Remove the top N items from the stack. See also: drop and clear. Since 1.5.0"},{"location":"asl-reference/nip.html#nip","title":"nip","pageTitle":"nip","text":""},{"location":"asl-reference/nip.html#signature","title":"Signature","pageTitle":"nip","text":"a b -- b"},{"location":"asl-reference/nip.html#summary","title":"Summary","pageTitle":"nip","text":"Remove the second item on the stack. See also: drop and clear. Since 1.5.0"},{"location":"asl-reference/nlist.html#nlist","title":"nlist","pageTitle":"nlist","text":""},{"location":"asl-reference/nlist.html#signature","title":"Signature","pageTitle":"nlist","text":"aN ... a0 N -- aN List(aN-1 ... a0)"},{"location":"asl-reference/nlist.html#summary","title":"Summary","pageTitle":"nlist","text":"Create a list with the top N items on the stack. See also: list. Since: 1.5.0"},{"location":"asl-reference/over.html#over","title":"over","pageTitle":"over","text":""},{"location":"asl-reference/over.html#signature","title":"Signature","pageTitle":"over","text":"a b -- a b a"},{"location":"asl-reference/over.html#summary","title":"Summary","pageTitle":"over","text":"Duplicates the second item on the stack and pushes the copy to the top. See also: 2over and dup."},{"location":"asl-reference/pick.html#pick","title":"pick","pageTitle":"pick","text":""},{"location":"asl-reference/pick.html#signature","title":"Signature","pageTitle":"pick","text":"aN ... a0 N -- aN ... a0 aN"},{"location":"asl-reference/pick.html#summary","title":"Summary","pageTitle":"pick","text":"Pick an item in the stack and put a copy on the top. See also: dup. Since: 1.5.0"},{"location":"asl-reference/random.html#random","title":"random","pageTitle":"random","text":""},{"location":"asl-reference/random.html#signature","title":"Signature","pageTitle":"random","text":"-- TimeSeriesExpr"},{"location":"asl-reference/random.html#summary","title":"Summary","pageTitle":"random","text":"Generate a time series that appears to be random noise for the purposes of creating sample data to experiment with expressions. To ensure that the line is deterministic and reproducible it actually is based on a hash of the timestamp. Each datapoint is a value between 0.0 and 1.0. See also :srandom."},{"location":"asl-reference/roll.html#roll","title":"roll","pageTitle":"roll","text":""},{"location":"asl-reference/roll.html#signature","title":"Signature","pageTitle":"roll","text":"aN ... a0 N -- aN-1 ... a0 aN"},{"location":"asl-reference/roll.html#summary","title":"Summary","pageTitle":"roll","text":"Rotate an item in the stack and put it on the top. Since: 1.5.0"},{"location":"asl-reference/rot.html#rot","title":"rot","pageTitle":"rot","text":""},{"location":"asl-reference/rot.html#signature","title":"Signature","pageTitle":"rot","text":"a b * -- b * a"},{"location":"asl-reference/rot.html#summary","title":"Summary","pageTitle":"rot","text":"Rotate the stack so that the item at the bottom is now at the top. To rotate in the reverse direction use the -rot operator."},{"location":"asl-reference/sdes.html#sdes","title":"sdes","pageTitle":"sdes","text":""},{"location":"asl-reference/sdes.html#signature","title":"Signature","pageTitle":"sdes","text":"TimeSeriesExpr training:Int alpha:Double beta:Double -- TimeSeriesExpr"},{"location":"asl-reference/sdes-fast.html#sdes-fast","title":"sdes-fast","pageTitle":"sdes-fast","text":""},{"location":"asl-reference/sdes-fast.html#signature","title":"Signature","pageTitle":"sdes-fast","text":"TimeSeriesExpr -- TimeSeriesExpr"},{"location":"asl-reference/sdes-fast.html#summary","title":"Summary","pageTitle":"sdes-fast","text":"Helper for computing sliding DES using settings to quickly adjust to the input line. See recommended values for more information."},{"location":"asl-reference/sdes-slow.html#sdes-slow","title":"sdes-slow","pageTitle":"sdes-slow","text":""},{"location":"asl-reference/sdes-slow.html#signature","title":"Signature","pageTitle":"sdes-slow","text":"TimeSeriesExpr -- TimeSeriesExpr"},{"location":"asl-reference/sdes-slow.html#summary","title":"Summary","pageTitle":"sdes-slow","text":"Helper for computing sliding DES using settings to quickly adjust to the input line. See recommended values for more information."},{"location":"asl-reference/sdes-slower.html#sdes-slower","title":"sdes-slower","pageTitle":"sdes-slower","text":""},{"location":"asl-reference/sdes-slower.html#signature","title":"Signature","pageTitle":"sdes-slower","text":"TimeSeriesExpr -- TimeSeriesExpr"},{"location":"asl-reference/sdes-slower.html#summary","title":"Summary","pageTitle":"sdes-slower","text":"Helper for computing sliding DES using settings to quickly adjust to the input line. See recommended values for more information."},{"location":"asl-reference/set.html#set","title":"set","pageTitle":"set","text":""},{"location":"asl-reference/set.html#signature","title":"Signature","pageTitle":"set","text":"k v --"},{"location":"asl-reference/set.html#summary","title":"Summary","pageTitle":"set","text":"Pop the key and value off of the stack and store the value such that it can be retrieved by using get with the key."},{"location":"asl-reference/srandom.html#srandom","title":"srandom","pageTitle":"srandom","text":""},{"location":"asl-reference/srandom.html#signature","title":"Signature","pageTitle":"srandom","text":"seed:Int -- TimeSeriesExpr"},{"location":"asl-reference/srandom.html#summary","title":"Summary","pageTitle":"srandom","text":"Generate a time series that appears to be random noise for the purposes of creating sample data to experiment with expressions. To ensure that the line is deterministic and reproducible it actually is based on a hash of the timestamp. Different seed values can be used to generate distinct noise lines. Each datapoint is a value between 0.0 and 1.0. See also :random. Since 1.6.0"},{"location":"asl-reference/sset.html#sset","title":"sset","pageTitle":"sset","text":""},{"location":"asl-reference/sset.html#signature","title":"Signature","pageTitle":"sset","text":"v k --"},{"location":"asl-reference/sset.html#summary","title":"Summary","pageTitle":"sset","text":"Swap and set. For more information see set."},{"location":"asl-reference/swap.html#swap","title":"swap","pageTitle":"swap","text":""},{"location":"asl-reference/swap.html#signature","title":"Signature","pageTitle":"swap","text":"a b -- b a"},{"location":"asl-reference/swap.html#summary","title":"Summary","pageTitle":"swap","text":"Swap the top two items on the stack."},{"location":"asl-reference/tuck.html#tuck","title":"tuck","pageTitle":"tuck","text":""},{"location":"asl-reference/tuck.html#signature","title":"Signature","pageTitle":"tuck","text":"a b -- b a b"},{"location":"asl-reference/tuck.html#summary","title":"Summary","pageTitle":"tuck","text":"Copy the first item on the stack below the second item. Equivalent of writing :swap,:over. Since 1.5.0"},{"location":"spectator/registry.html#registry","title":"Registry","pageTitle":"Registry","text":"There are a few basic concepts you need to learn to use Spectator. The registry is the main class for managing a set of meters. A meter is a class for collecting a set of measurements about your application."},{"location":"spectator/registry.html#choosing-an-implementation","title":"Choosing an Implementation","pageTitle":"Registry","text":"The core spectator library, spectator-api, comes with the following registry implementations:  Class Dependency Description DefaultRegistry spectator-api Updates local counters, frequently used with unit tests. NoopRegistry spectator-api Does nothing, tries to make operations as cheap as possible. This implementation is typically used to help understand the overhead being created due to instrumentation. It can also be useful in testing to help ensure that no side effects were introduced where the instrumentation is now needed in order for the application for function properly. ServoRegistry spectator-reg-servo Map to servo library. This is the implementation typically used at Netflix to report data into Atlas. MetricsRegistry spectator-reg-metrics3 Map to metrics3 library. This implementation is typically used for reporting to local files, JMX, or other backends like Graphite. Note that it uses a hierarchical naming scheme rather than the dimensional naming used by Spectator, so the names will get flattened when mapped to this registry.  It is recommended for libraries to write code against the Registry interface and allow the implementation to get injected by the user of the library. The simplest way is to accept the registry via the constructor, for example: public class HttpServer {\n  public HttpServer(Registry registry) {\n    // use registry to collect measurements\n  }\n} The user of the class can then provide the implementation: Registry registry = new DefaultRegistry();\nHttpServer server = new HttpServer(registry); More complete examples can be found on the testing page or in the spectator-examples repo."},{"location":"spectator/registry.html#working-with-ids","title":"Working With Ids","pageTitle":"Registry","text":"Spectator is primarily intended for collecting data for dimensional time series backends like Atlas. The ids used for looking up a meter in the registry consist of a name and set of tags. Ids will be consumed many times by users after the data has been reported so they should be chosen with some care and thought about how they will get used. See the conventions page for some general guidelines. Ids are created via the registry, for example: Id id = registry.createId(\"server.requestCount\"); The ids are immutable so they can be freely passed around and used in a concurrent context. Tags can be added when an id is created: Id id = registry.createId(\"server.requestCount\", \"status\", \"2xx\", \"method\", \"GET\"); Or by using withTag and withTags on an existing id: public class HttpServer {\n  private final Id baseId;\n\n  public HttpServer(Registry registry) {\n    baseId = registry.createId(\"server.requestCount\");\n  }\n\n  private void handleRequestComplete(HttpRequest req, HttpResponse res) {\n    // Remember Id is immutable, withTags will return a copy with the\n    // the additional metadata\n    Id reqId = baseId.withTags(\n      \"status\", res.getStatus(),\n      \"method\", req.getMethod().name());\n    registry.counter(reqId).increment();\n  }\n\n  private void handleRequestError(HttpRequest req, Throwable t) {\n    // Can also be added individually using `withTag`. However, it is better\n    // for performance to batch modifications using `withTags`.\n    Id reqId = baseId\n      .withTag(\"error\",  t.getClass().getSimpleName())\n      .withTag(\"method\", req.getMethod().name());\n    registry.counter(reqId).increment();\n  }\n}"},{"location":"spectator/registry.html#collecting-measurements","title":"Collecting Measurements","pageTitle":"Registry","text":"Once you have an id, the registry can be used to get an instance of a meter to record a measurement. Meters can roughly be categorized in two groups:"},{"location":"spectator/registry.html#active","title":"Active","pageTitle":"Registry","text":"Active meters are ones that are called directly when some event occurs. There are three basic types supported: Counters: measures how often something is occuring. This will be reported to backend systems as a rate per second. For example, number of requests processed by web server. Timers: measures how long something took. For example, latency of requests processed by a web server. Distribution Summaries: measures the size of something. For example, entity sizes for requests processed by a web server."},{"location":"spectator/registry.html#passive","title":"Passive","pageTitle":"Registry","text":"Passive meters are ones where the registry just has a reference to get the value when needed. For example, the number of current connections on a web server or the number threads that are currently in use. These will be gauges."},{"location":"spectator/timers.html#timers","title":"Timers","pageTitle":"Timers","text":"A timer is used to measure how long some event is taking. Two types of timers are supported: Timer: for frequent short duration events. LongTaskTimer: for long running tasks. The long duration timer is setup so that you can track the time while an event being measured is still running. A regular timer just records the duration and has no information until the task is complete. As an example, consider a chart showing request latency to a typical web server. The expectation is many short requests so the timer will be getting updated many times per second.  Now consider a background process to refresh metadata from a data store. For example, Edda caches AWS resources such as instances, volumes, auto-scaling groups etc. Normally all data can be refreshed in a few minutes. If the AWS services are having problems it can take much longer. A long duration timer can be used to track the overall time for refreshing the metadata. The charts below show max latency for the refresh using a regular timer and a long task timer. Regular timer, note that the y-axis is using a logarithmic scale:  Long task timer:"},{"location":"spectator/timers.html#timer","title":"Timer","pageTitle":"Timers","text":"To get started create an instance using the registry: Java public class TimerExample {\n\n  private final Registry registry;\n  private final Timer requestLatency;\n\n  @Inject\n  public TimerExample(Registry registry) {\n    this.registry = registry;\n    requestLatency = registry.timer(\"server.requestLatency\");\n  } Python class TimerExample:\n\n    def __init__(self, registry=GlobalRegistry):\n        self._registry = registry\n        self._requestLatency = registry.timer('server.requestLatency') Then wrap the call you need to measure, preferably using a lambdaa with statement: Java public Response handleUsingLambda(Request request) throws Exception {\n  return requestLatency.record(() -> handleImpl(request));\n} Python def handle_using_with(self, request):\n    with self._requestLatency.stopwatch():\n        return self._handle_impl(request) The lambda variants will handle exceptions for you and ensure the record happens as part of a finally block using the monotonic time. It could also have been done more explicitly like: Java public Response handleExplicitly(Request request) {\n  final long start = registry.clock().monotonicTime();\n  try {\n    return handleImpl(request);\n  } finally {\n    final long end = registry.clock().monotonicTime();\n    requestLatency.record(end - start, TimeUnit.NANOSECONDS);\n  }\n} Python def handle_explicitly(self, request):\n    start = self._registry.clock().monotonic_time()\n    try:\n        return self._handle_impl(request)\n    finally:\n        end = self._registry.clock().monotonic_time()\n        self._requestLatency.record(end - start) This example uses the clock from the registry which can be useful for testing if you need to control the timing. In actual usage it will typically get mapped to the system clock. It is recommended to use a monotonically increasing source for measuring the times to avoid occasionally having bogus measurements due to time adjustments. For more information see the Clock documentation."},{"location":"spectator/gauges.html#gauges","title":"Gauges","pageTitle":"Gauges","text":"A gauge is a value that is sampled at some point in time. Typical examples for gauges would be the size of a queue or number of threads in the running state. Since gauges are not updated inline when a state change occurs, there is no information about what might have occurred between samples. Consider monitoring the behavior of a queue of tasks. If the data is being collected once a minute, then a gauge for the size will show the size when it was sampled. The size may have been much higher or lower at some point during interval, but that is not known."},{"location":"spectator/gauges.html#polled-gauges","title":"Polled Gauges","pageTitle":"Gauges","text":"The most common use of gauges is by registering a hook with Spectator so that it will poll the values in the background. This is done by using the PolledMeter helper class. A polled gauge is registered by passing in an id, a reference to the object, and a function to get or compute a numeric value based on the object. Note that a gauge should only be registered once, not on each update. Consider this example of a web server tracking the number of connections: class HttpServer {\n  // Tracks the number of current connections to the server\n  private AtomicInteger numConnections;\n\n  public HttpServer(Registry registry) {\n    numConnections = PolledMeter.using(registry)\n      .withName(\"server.numConnections\")\n      .monitorValue(new AtomicInteger(0));\n  }\n\n  public void onConnectionCreated() {\n    numConnections.incrementAndGet();\n    ...\n  }\n\n  public void onConnectionClosed() {\n    numConnections.decrementAndGet();\n    ...\n  }\n\n  ...\n} The spectator registry will keep a weak reference to the object. If the object is garbage collected, then it will automatically drop the registration. In the example above, the registry will have a weak reference to numConnections and the server instance will have a strong reference to numConnections. If the server instance goes away, then the gauge will as well. When multiple gauges are registered with the same id the reported value will be the sum of the matches. For example, if multiple instances of the HttpServer class were created on different ports, then the value server.numConnections would be the total number of connections across all server instances. If a different behavior is desired, then ensure your usage does not perform multiple registrations. There are several different ways to register a gauge:"},{"location":"spectator/gauges.html#using-number","title":"Using Number","pageTitle":"Gauges","text":"A gauge can also be created based on an implementation of Number. Note the number implementation should be thread safe. For example: AtomicInteger size = new AtomicInteger();\nPolledMeter.using(registry)\n  .withName(\"queue.size\")\n  .monitorValue(size); The call will return the Number so the registration can be inline on the assignment: AtomicInteger size = PolledMeter.using(registry)\n  .withName(\"queue.size\")\n  .monitorValue(size); Updates to the value are preformed by updating the number instance directly."},{"location":"spectator/gauges.html#using-lambda","title":"Using Lambda","pageTitle":"Gauges","text":"Specify a lambda that takes the object as parameter. public class Queue {\n\n  @Inject\n  public Queue(Registry registry) {\n    PolledMeter.using(registry)\n      .withName(\"queue.size\")\n      .monitorValue(this, Queue::size);\n  }\n\n  ...\n} !!! warning Be careful to avoid creating a reference to the object in the lambda. It will prevent garbage collection and can lead to a memory leak in the application. For example, by calling size without using the passed in object there will be a reference to this: ```\nPolledMeter.using(registry).withName(\"queue.size\").monitorValue(this, obj -> size());\n```"},{"location":"spectator/gauges.html#collection-sizes","title":"Collection Sizes","pageTitle":"Gauges","text":"For classes that implement Collection or Map there are helpers: Queue queue = new LinkedBlockingQueue();\nPolledMeter.using(registry)\n  .withName(\"queue.size\")\n  .monitorSize(queue);\n\nMap<String, String> cache = new ConcurrentMap<>();\nPolledMeter.using(registry)\n  .withName(\"cache.size\")\n  .monitorSize(cache);"},{"location":"spectator/gauges.html#monotonic-counters","title":"Monotonic Counters","pageTitle":"Gauges","text":"A common technique used by some libraries is to expose a monotonically increasing counter that represents the number of events since the system was initialized. An example of that in the JDK is ThreadPoolExecutor.getCompletedTaskCount() which returns the number of completed tasks on the thread pool. For sources like this the monitorMonotonicCounter method can be used: // For an implementation of Number\nLongAdder tasks = new LongAdder();\nPolledMeter.using(registry)\n  .withName(\"pool.completedTasks\")\n  .monitorMonotonicCounter(tasks);\n\n// Or using a lambda\nThreadPoolExecutor executor = ...\nPolledMeter.using(registry)\n  .withName(\"pool.completedTasks\")\n  .monitorMonotonicCounter(executor, ThreadPoolExecutor::getCompletedTaskCount); For thread pools specifically, there are better options for getting standard metrics. See the docs for the Thread Pools extension for more information."},{"location":"spectator/clock.html#clock","title":"Clock","pageTitle":"Clock","text":"When taking measurements or working with timers it is recommended to use the clock interface. It provides two methods for measuring time:"},{"location":"spectator/clock.html#wall-time","title":"Wall Time","pageTitle":"Clock","text":"This is what most users think of for time. It can be used to get the current time like what you would see on a wall clock. In most cases when not running in tests this will call System.currentTimeMillis(). Note that the values returned by this method may not be monontonically increasing. Just like a clock on your wall, this value can go back in time or jump forward at unpredictable intervals if someone sets the time. On many systems ntpd will be constantly keeping the time synced up with an authoritative source. With spectator, the clock is typically accessed via the regstry. Example of usage: // Current time in milliseconds since the epoch\nlong currentTime = registry.clock().wallTime();"},{"location":"spectator/clock.html#monotonic-time","title":"Monotonic Time","pageTitle":"Clock","text":"While it is good in general for the wall clock to show the correct time, the unpredictable changes mean it is not a good choice for measuring how long an operation took. Consider a simple example of measuring request latency on a server: long start = registry.clock().wallTime();\nhandleRequest(request, response);\nlong end = registry.clock().wallTime();\nreqLatencyTimer.record(end - start, TimeUnit.MILLISECONDS); If ntp fixes the server time between start and end, then the recorded latency will be wrong. Spectator will protect against obviously wrong measurements like negative latencies by dropping those values when they are recorded. However, the change could incorrectly shorten or lengthen the measured latency. The clock interface also provides access to a monotonic source that is only useful for measuring elapsed time, for example: long start = registry.clock().monotonicTime();\nhandleRequest(request, response);\nlong end = registry.clock().monotonicTime();\nreqLatencyTimer.record(end - start, TimeUnit.NANOSECONDS); In most cases this will map to System.nanoTime(). Note the actual value returned is not meaningful unless compared with another sample to get a delta."},{"location":"spectator/testing.html#testing","title":"Testing","pageTitle":"Testing","text":"Testing should be relatively straightforward if you are using injection for the registry. Consider a sample class: public class Foo {\n\n  private final Counter counter;\n\n  @Inject\n  public Foo(Registry registry) {\n    counter = registry.counter(\"foo\");\n  }\n\n  public void doSomething() {\n    counter.increment();\n  }\n} Tests will typically want to use an isolated instance of the DefaultRegistry."},{"location":"spectator/testing.html#simple-test","title":"Simple Test","pageTitle":"Testing","text":"A basic standalone test class would look something like: public class FooTest {\n\n  private Registry registry;\n  private Foo foo;\n\n  @Before\n  public void init() {\n    registry = new DefaultRegistry();\n    foo = new Foo(registry);\n  }\n\n  @Test\n  public void doSomething() {\n    foo.doSomething();\n    Assert.assertEquals(1, registry.counter(\"foo\").count());\n  }\n}"},{"location":"spectator/testing.html#guice-test","title":"Guice Test","pageTitle":"Testing","text":"If using guice, then the TestModule can be used: public class FooTest {\n\n  private Registry registry;\n  private Foo foo;\n\n  @Before\n  public void init() {\n    Injector injector = Guice.createInjector(new TestModule());\n    registry = injector.getInstance(Registry.class);\n    foo = injector.getInstance(Foo.class);\n  }\n\n  @Test\n  public void doSomething() {\n    foo.doSomething();\n    Assert.assertEquals(1, registry.counter(\"foo\").count());\n  }\n}"},{"location":"spectator/conventions.html#names","title":"Names","pageTitle":"Names","text":"Quick summary: Names Describe the measurement being collected Use camel case Static Succinct Tags Should be used for dimensional drill-down Be careful about combinatorial explosion Tag keys should be static Use id to distinguish between instances Use base units"},{"location":"spectator/conventions.html#describe-the-measurement","title":"Describe the measurement","pageTitle":"Names","text":""},{"location":"spectator/conventions.html#use-camel-case","title":"Use camel case","pageTitle":"Names","text":"The main goal here is to promote consistency which makes it easier for users. The choice of style is somewhat arbitrary, camel case was chosen because: Used by snmp Used by java It was the most common in use at Netflix when this guideline was added The exception to this rule is where there is an established common case. For example with Amazon regions it is preferred to use us-east-1 rather than usEast1 as it is the more common form."},{"location":"spectator/conventions.html#static","title":"Static","pageTitle":"Names","text":"There shouldn’t be any dynamic content that goes into a metric name. Metric names and associated tag keys are how users will interact with the data being produced."},{"location":"spectator/conventions.html#succinct","title":"Succinct","pageTitle":"Names","text":"Long names should be avoided."},{"location":"spectator/conventions.html#tags","title":"Tags","pageTitle":"Names","text":"Historically tags have been used to play one of two roles: Dimensions: dimensions are the primary use and it allows the data to be sliced and diced so it is possible to drill down into the data. Namespace: similar to packages in Java in this mode it would be used to group related data. This type of usage is discouraged. As a general rule it should be possible to use the name as a pivot. This means that if just the name is selected, then the user can drill down using other dimensions and be able to reason about the value being shown. As a concrete example, suppose we have two metrics: The number of threads currently in a thread pool. The number of rows in a database table."},{"location":"spectator/conventions.html#bad-approach","title":"Bad approach","pageTitle":"Names","text":"Id poolSize = registry.createId(\"size\")\n  .withTag(\"class\", \"ThreadPool\")\n  .withTag(\"id\", \"server-requests\");\n  \nId poolSize = registry.createId(\"size\")\n  .withTag(\"class\", \"Database\")\n  .withTag(\"table\", \"users\"); In this approach, if I select the name, size, it will match both the version for ThreadPool and Database classes. So you would get a value that is the an aggregate of the number of threads and the number of items in a database."},{"location":"spectator/conventions.html#recommended","title":"Recommended","pageTitle":"Names","text":"Id poolSize = registry.createId(\"threadpool.size\")\n  .withTag(\"id\", \"server-requests\");\n  \nId poolSize = registry.createId(\"db.size\")\n  .withTag(\"table\", \"users\"); This variant provides enough context so that if just the name is selected the value can be reasoned about and is at least potentially meaningful. For example if I select threadpool.size I can see the total number of threads in all pools. Then I can group by or select an id to drill down further."},{"location":"spectator/servo.html#servo-comparison","title":"Servo Comparison","pageTitle":"Servo Comparison","text":"Servo is an alternative client monitoring library that is also developed by Netflix. Originally spectator was an experiment for a simpler API that wrapped servo. It was done as a separate project to avoid breaking backwards compatibility for servo. From a user perspective, both will be supported for a long time, but most of our efforts for future improvement will go to spectator. For new code it is recommended to use the spectator API. If running [at Netflix](Netflix Integration) the correct bindings will be in place for both servo and spectator."},{"location":"spectator/servo.html#differences","title":"Differences","pageTitle":"Servo Comparison","text":"This section provides a quick summary of the differences."},{"location":"spectator/servo.html#simpler-api","title":"Simpler API","pageTitle":"Servo Comparison","text":"Servo gives the user a lot of control, but this makes it hard to use correctly. For example, to create a counter the user needs to understand the tradeoffs and choose between: BasicCounter DynamicCounter ContextualCounter StepCounter Further each of these can impact how data is reported to observers. The spectator API focuses on the constructs a user needs to instrument the code. In spectator the user would always use the registry to create a Counter. The implementation details are left up to the registry. The registration is simpler as well to avoid common pitfalls when using servo like overwriting a registered object."},{"location":"spectator/servo.html#more-focused","title":"More Focused","pageTitle":"Servo Comparison","text":"The goal of spectator is instrumenting code to send to a dimensional time-series system like Atlas. Servo has goals of staying compatible with a number of legacy libraries and naming formats, exposing data to JMX, etc. Examples of how this influences decisions: No support for non-numeric data. Servo supported this for exposing to JMX. Exposing the numeric data registered in spectator to JMX can be done using a registry that supports it, but there is no goal to be a general interface for exposing arbitrary data in JMX. No support for customizeable time units when reporting timer data. Base units should always be used for reporting and conversions can be performed in the presentation layer if needed. It also avoids a lot of the confusion around the timer unit for the data and issues like creating aggregates that are meaningless because of mixed units. It is better to have a simple way to get correct and easy to reason about data to the backend than many options. If you want more knobs, then you can use Servo."},{"location":"spectator/servo.html#di-friendly","title":"DI Friendly","pageTitle":"Servo Comparison","text":"When servo was originally written dependency injection was not heavily used at Netflix. Further servo needed to stay compatible with a number of use-cases that were heavily static. While spectator does have a static registry that can be used, the recommended way is to create a registry and inject it either manually or via a framework into the classes that need it. This also makes it much easier to test in isolation."},{"location":"spectator/servo.html#migration","title":"Migration","pageTitle":"Servo Comparison","text":"If you want to migrate from the servo API to the spectator API, then this section provides some guides on how servo constructs can be ported over. The sub-sections are the class names of monitor types supported by servo. For users at Netflix, note we are not actively pushing teams to migrate or do any additional work. Servo is still supported and if it works for your use-cases feel free to continue using it."},{"location":"spectator/servo.html#registration","title":"Registration","pageTitle":"Servo Comparison","text":"First read through the servo docs on registration. In servo if you have a class like: public class Foo {\n\n  private AtomicInteger gauge;\n  private Counter counter;\n\n  public Foo(String id) {\n    gauge = new AtomicInteger();\n    counter = new BasicCounter(MonitorConfig.builder(\"counter\").build());\n    Monitors.registerObject(id, this);\n  }\n\n  @Monitor(name = \"gauge\", type = DataSourceType.GAUGE)\n  private int gauge() {\n    return gauge.get();\n  }\n\n  public void doSomething() {\n    ...\n  }\n} The state of the class is in the member variables of an instance of Foo. If multiple instances of class Foo are created with the same value for id, then the last one will overwrite the others for the registration. So the values getting reported will only be from the last instance registered. Also the registry has a reference to the instance of Foo so it will never go away. For counters and timers one way to get around this is to use DynamicCounter and DynamicTimer respectively. Those classes will automatically handle the registration and expire if there is no activity. They also get used for cases where the set of dimensions is not known up front. Gauges need to sample the state of something so they need to have a reference to an object that contains the state. So the user would need to ensure that only a single copy was registered leading to patterns like: class Foo {\n\n  private static class FooStats {\n\n    private AtomicInteger gauge;\n    private Counter counter;\n\n    public FooStats(String id) {\n      gauge = new AtomicInteger();\n      counter = new BasicCounter(MonitorConfig.builder(\"counter\").build());\n      Monitors.registerObject(id, this);\n    }\n\n    @Monitor(name = \"gauge\", type = DataSourceType.GAUGE)\n    private int gauge() {\n      return gauge.get();\n    }\n  }\n\n  private static ConcurrentHashMap<String, FooStats> STATS =\n    new ConcurrentHashMap<>();\n\n  private final FooStats stats;\n\n  public Foo(String id) {\n    stats = STATS.computeIfAbsent(id, (i) -> new FooStats(i));\n  }\n\n  public void doSomething() {\n    ...\n    stats.update();\n  }\n} This ensures that there is a single copy for a given id. In spectator this example would look like: public class Foo {\n\n  private AtomicInteger gauge;\n  private Counter counter;\n\n  public Foo(Registry registry, String id) {\n    Id gaugeId = registry.createId(\"gauge\").withTag(\"id\", id);\n    gauge = registry.gauge(gaugeId, new AtomicInteger());\n    counter = registry.counter(\"counter\", \"id\", id);\n  }\n\n  public void doSomething() {\n    ...\n  }\n} Everything using the same registry will get the same counter instance if the same id is used. For the gauge the registry will keep a weak reference and will sum the values if multiple instances are present. Since it is a weak reference, nothing will prevent an instance of Foo from getting garbage collected."},{"location":"spectator/servo.html#annotations","title":"Annotations","pageTitle":"Servo Comparison","text":"Annotations are not supported, use the appropriate meter type: DataSourceType Spectator Alternative COUNTER Counter Usage GAUGE Gauge Usage INFORMATIONAL Not supported"},{"location":"spectator/servo.html#basiccounter","title":"BasicCounter","pageTitle":"Servo Comparison","text":"See the general overview of registration differences and summary of counter usage. Servo: public class Foo {\n  private final Counter c =\n    new BasicCounter(MonitorConfig.builder(\"name\").build());\n\n  public Foo(String id) {\n    Monitors.registerObject(id, this);\n  }\n\n  public void doSomething() {\n    c.increment();\n  }\n} Spectator: public class Foo {\n  private final Counter c;\n\n  @Inject\n  public Foo(Registry registry, String id) {\n    c = registry.counter(\"name\", \"id\", id);\n  }\n\n  public void doSomething() {\n    c.increment();\n  }\n}"},{"location":"spectator/servo.html#basicgauge","title":"BasicGauge","pageTitle":"Servo Comparison","text":"See the general overview of registration differences and summary of gauge usage. Servo: public class Foo {\n  private final BasicGauge g = new BasicGauge(\n    MonitorConfig.builder(\"name\").build(),\n    this::getCurrentValue);\n\n  public Foo(String id) {\n    Monitors.registerObject(id, this);\n  }\n} Spectator: public class Foo {\n  @Inject\n  public Foo(Registry registry, String id) {\n    Id gaugeId = registry.createId(\"name\").withTag(\"id\", id);\n    registry.gauge(gaugeId, this, Foo::getCurrentValue);\n  }\n}"},{"location":"spectator/servo.html#basictimer","title":"BasicTimer","pageTitle":"Servo Comparison","text":"See the general overview of registration differences and summary of timer usage. Note in spectator the reported unit for timers is always seconds and cannot be changed. Seconds is the base unit and other units should only be used as a presentation detail. Servo allows the unit to be customized and defaults to milliseconds. Servo: public class Foo {\n  private final Timer t = new BasicTimer(\n    MonitorConfig.builder(\"name\").build(), TimeUnit.SECONDS);\n\n  public Foo(String id) {\n    Monitors.registerObject(id, this);\n  }\n\n  public void doSomething() {\n    Stopwatch s = t.start();\n    try {\n      ...\n    } finally {\n      s.stop();\n    }\n  }\n} Spectator: public class Foo {\n  private final Timer t;\n\n  @Inject\n  public Foo(Registry registry, String id) {\n    t = registry.timer(\"name\", \"id\", id);\n  }\n\n  public void doSomething() {\n    t.record(() -> {\n      ...\n    });\n  }\n}"},{"location":"spectator/servo.html#basicdistributionsummary","title":"BasicDistributionSummary","pageTitle":"Servo Comparison","text":"See the general overview of registration differences and summary of distribution summary usage. Servo: public class Foo {\n  private final BasicDistributionSummary s = new BasicDistributionSummary(\n    MonitorConfig.builder(\"name\").build());\n\n  public Foo(String id) {\n    Monitors.registerObject(id, this);\n  }\n\n  public void doSomething() {\n    ...\n    s.record(getValue());\n  }\n} Spectator: public class Foo {\n  private final DistributionSummary s;\n\n  @Inject\n  public Foo(Registry registry, String id) {\n    s = registry.distributionSummary(\"name\", \"id\", id);\n  }\n\n  public void doSomething() {\n    ...\n    s.record(getValue());\n  }\n}"},{"location":"spectator/servo.html#basicinformational","title":"BasicInformational","pageTitle":"Servo Comparison","text":"Not supported, see overview of differences."},{"location":"spectator/servo.html#basicstopwatch","title":"BasicStopwatch","pageTitle":"Servo Comparison","text":"There isn’t an explicit stopwatch class in spectator. Just use a timing call directly. Servo: public void doSomething() {\n    Stopwatch s = timer.start();\n    try {\n      ...\n    } finally {\n      s.stop();\n    }\n  } Spectator: public void doSomething() {\n    final long s = System.nanoTime();\n    try {\n      ...\n    } finally {\n      timer.record(System.nanoTime() - s, TimeUnit.NANOSECONDS);\n    }\n  }"},{"location":"spectator/servo.html#buckettimer","title":"BucketTimer","pageTitle":"Servo Comparison","text":"See the general overview of registration differences and summary of sandbox documentation. Note in spectator BucketTimer is provided in the sandbox extension library and may change in future as we gain more experience using it. Servo: public class Foo {\n  private final Timer t = new BucketTimer(\n    MonitorConfig.builder(\"name\").build(),\n    new BucketConfig.Builder()\n      .withTimeUnit(TimeUnit.MILLISECONDS)\n      .withBuckets(new long[] { 500, 2500, 5000, 10000 })\n      .build());\n\n  public Foo(String id) {\n    Monitors.registerObject(id, this);\n  }\n\n  public void doSomething() {\n    Stopwatch s = t.start();\n    try {\n      ...\n    } finally {\n      s.stop();\n    }\n  }\n} Spectator: public class Foo {\n  private final Timer t;\n\n  @Inject\n  public Foo(Registry registry, String id) {\n    Id timerId = registry.createId(\"name\", \"id\", id);\n    BucketFunction f = BucketFunctions.latency(10, TimeUnit.SECONDS);\n    t = BucketTimer.get(registry, timerId, f);\n  }\n\n  public void doSomething() {\n    t.record(() -> {\n      ...\n    });\n  }\n}"},{"location":"spectator/servo.html#contextualcounter","title":"ContextualCounter","pageTitle":"Servo Comparison","text":"Not supported. A fixed tag list for the context is too rigid and this class was never used much at Netflix. Future work being looked at in issue-180."},{"location":"spectator/servo.html#contextualtimer","title":"ContextualTimer","pageTitle":"Servo Comparison","text":"Not supported. A fixed tag list for the context is too rigid and this class was never used much at Netflix. Future work being looked at in issue-180."},{"location":"spectator/servo.html#doublegauge","title":"DoubleGauge","pageTitle":"Servo Comparison","text":"See the general overview of registration differences and summary of gauge usage. Servo: public class Foo {\n  private final DoubleGauge g = new DoubleGauge(\n    MonitorConfig.builder(\"name\").build());\n\n  public Foo(String id) {\n    Monitors.registerObject(id, this);\n  }\n} Spectator: import com.google.common.util.concurrent.AtomicDouble;\n\npublic class Foo {\n  private final AtomicDouble v;\n\n  @Inject\n  public Foo(Registry registry, String id) {\n    Id gaugeId = registry.createId(\"name\").withTag(\"id\", id);\n    v = registry.gauge(gaugeId, new AtomicDouble());\n  }\n}"},{"location":"spectator/servo.html#durationtimer","title":"DurationTimer","pageTitle":"Servo Comparison","text":"See the general overview of registration differences and summary of timer usage. Servo: public class Foo {\n  private final DurationTimer t = new DurationTimer(\n    MonitorConfig.builder(\"name\").build());\n\n  public Foo(String id) {\n    Monitors.registerObject(id, this);\n  }\n} Spectator: public class Foo {\n  private final LongTaskTimer t;\n\n  @Inject\n  public Foo(Registry registry, String id) {\n    t = registry.longTaskTimer(\"name\", \"id\", id);\n  }\n}"},{"location":"spectator/servo.html#dynamiccounter","title":"DynamicCounter","pageTitle":"Servo Comparison","text":"See the general overview of registration differences and summary of counter usage. Servo: public class Foo {\n\n  private final String id;\n\n  public Foo(String id) {\n    this.id = id;\n  }\n\n  public void doSomething(Context ctxt) {\n    DynamicCounter.increment(\"staticId\", \"id\", id);\n    DynamicCounter.increment(\"dynamicId\", \"id\", id, \"foo\", ctxt.getFoo());\n  }\n} Spectator: public class Foo {\n  private final Registry registry;\n  private final String id;\n  private final Counter staticCounter;\n  private final Id dynamicId;\n\n  @Inject\n  public Foo(Registry registry, String id) {\n    this.registry = registry;\n    this.id = id;\n    staticCounter = registry.counter(\"staticId\", \"id\", id);\n    dynamicId = registry.createId(\"dynamicId\", \"id\", id);\n  }\n\n  public void doSomething(Context ctxt) {\n    // Keeping the reference to the counter avoids additional allocations\n    // to create the id object and the lookup cost\n    staticCounter.increment();\n\n    // If the id is dynamic it must be looked up\n    registry.counter(\"dynamicId\", \"id\", id, \"foo\", ctxt.getFoo()).increment();\n\n    // This will update the same counter as the line above, but the base part\n    // of the id is precomputed to make it cheaper to construct the id.\n    registry.counter(dynamicId.withTag(\"foo\", ctxt.getFoo())).increment();\n  }\n}"},{"location":"spectator/servo.html#dynamictimer","title":"DynamicTimer","pageTitle":"Servo Comparison","text":"See the general overview of registration differences and summary of timer usage. Servo: public class Foo {\n\n  private final String id;\n  private final MonitorConfig staticId;\n\n  public Foo(String id) {\n    this.id = id;\n    staticId = MonitorConfig.builder(\"staticId\").withTag(\"id\", id).build();\n  }\n\n  public void doSomething(Context ctxt) {\n    final long d = ctxt.getDurationMillis();\n    DynamicTimer.record(staticId, TimeUnit.SECONDS, d, TimeUnit.MILLISECONDS);\n\n    MonitorConfig dynamicId = MonitorConfig.builder(\"dynamicId\")\n      .withTag(\"id\", id)\n      .withTag(\"foo\", ctxt.getFoo())\n      .build();\n    DynamicTimer.record(dynamicId, TimeUnit.SECONDS, d, TimeUnit.MILLISECONDS);\n  }\n} Spectator: public class Foo {\n  private final Registry registry;\n  private final String id;\n  private final Timer staticTimer;\n  private final Id dynamicId;\n\n  @Inject\n  public Foo(Registry registry, String id) {\n    this.registry = registry;\n    this.id = id;\n    staticTimer = registry.timer(\"staticId\", \"id\", id);\n    dynamicId = registry.createId(\"dynamicId\", \"id\", id);\n  }\n\n  public void doSomething(Context ctxt) {\n    final long d = ctxt.getDurationMillis();\n\n    // Keeping the reference to the timer avoids additional allocations\n    // to create the id object and the lookup cost\n    staticTimer.record(d, TimeUnit.MILLISECONDS);\n\n    // If the id is dynamic it must be looked up\n    registry.timer(\"dynamicId\", \"id\", id, \"foo\", ctxt.getFoo())\n      .record(d, TimeUnit.MILLISECONDS);\n\n    // This will update the same timer as the line above, but the base part\n    // of the id is precomputed to make it cheaper to construct the id.\n    registry.timer(dynamicId.withTag(\"foo\", ctxt.getFoo()))\n      .record(d, TimeUnit.MILLISECONDS);\n  }\n}"},{"location":"spectator/servo.html#longgauge","title":"LongGauge","pageTitle":"Servo Comparison","text":"See the general overview of registration differences and summary of gauge usage. Servo: public class Foo {\n  private final LongGauge g = new LongGauge(\n    MonitorConfig.builder(\"name\").build());\n\n  public Foo(String id) {\n    Monitors.registerObject(id, this);\n  }\n} Spectator: public class Foo {\n  private final AtomicLong v;\n\n  @Inject\n  public Foo(Registry registry, String id) {\n    Id gaugeId = registry.createId(\"name\").withTag(\"id\", id);\n    v = registry.gauge(gaugeId, new AtomicLong());\n  }\n}"},{"location":"spectator/servo.html#monitorconfig","title":"MonitorConfig","pageTitle":"Servo Comparison","text":"See the documentation on naming. Servo: MonitorConfig id = MonitorConfig.builder(\"name\")\n  .withTag(\"country\", \"US\")\n  .withTag(\"device\",  \"xbox\")\n  .build(); Spectator: Id id = registry.createId(\"name\")\n  .withTag(\"country\", \"US\")\n  .withTag(\"device\",  \"xbox\");\n\n// or\n\nId id = registry.createId(\"name\", \"country\", \"US\", \"device\", \"xbox\");"},{"location":"spectator/servo.html#monitoredcache","title":"MonitoredCache","pageTitle":"Servo Comparison","text":"Not supported because spectator does not have a direct dependency on guava. If there is enough demand an extension can be created."},{"location":"spectator/servo.html#numbergauge","title":"NumberGauge","pageTitle":"Servo Comparison","text":"See the general overview of registration differences and summary of gauge usage. Servo: public class Foo {\n  private final NumberGauge g = new NumberGauge(\n    MonitorConfig.builder(\"name\").build(), new AtomicLong());\n\n  public Foo(String id) {\n    Monitors.registerObject(id, this);\n  }\n} Spectator: public class Foo {\n  private final AtomicLong v;\n\n  @Inject\n  public Foo(Registry registry, String id) {\n    Id gaugeId = registry.createId(\"name\").withTag(\"id\", id);\n    v = registry.gauge(gaugeId, new AtomicLong());\n  }\n}"},{"location":"spectator/servo.html#statstimer","title":"StatsTimer","pageTitle":"Servo Comparison","text":"Not supported, see overview of differences."},{"location":"spectator-ext/jvm-buffer-pools.html#buffer-pools","title":"Buffer Pools","pageTitle":"Buffer Pools","text":"Buffer pools, such as direct byte buffers, can be monitored at a high level using the BufferPoolMXBean provided by the JDK."},{"location":"spectator-ext/jvm-buffer-pools.html#getting-started","title":"Getting Started","pageTitle":"Buffer Pools","text":"To get information about buffer pools in spectator just setup registration of standard MXBeans. Note, if you are building an app at Netflix this should happen automatically via the normal platform initialization. import com.netflix.spectator.api.Spectator;\nimport com.netflix.spectator.jvm.Jmx;\n\nJmx.registerStandardMXBeans(Spectator.registry());"},{"location":"spectator-ext/jvm-buffer-pools.html#metrics","title":"Metrics","pageTitle":"Buffer Pools","text":""},{"location":"spectator-ext/jvm-buffer-pools.html#jvm-buffer-count","title":"jvm.buffer.count","pageTitle":"Buffer Pools","text":"Gauge showing the current number of distinct buffers. Unit: count Dimensions: id: type of buffers. Value will be either direct for direct byte buffers or mapped for memory mapped files."},{"location":"spectator-ext/jvm-gc.html#garbage-collection","title":"Garbage Collection","pageTitle":"Garbage Collection","text":"The GC module registers with the notification emitter of the GarbageCollectorMXBean to provide some basic GC logging and metrics. Getting started Logging Metrics Alerting"},{"location":"spectator-ext/jvm-gc.html#getting-started","title":"Getting Started","pageTitle":"Garbage Collection","text":"For using it internally at Netflix see the Netflix integration guide, otherwise keep reading this section."},{"location":"spectator-ext/jvm-gc.html#requirements","title":"Requirements","pageTitle":"Garbage Collection","text":"This library relies on the notification emitter added in 7u4, but there are known issues prior to 7u40. There is also a regression impacting java 9 and higher, see #502 and JDK-8196325 for more information. For G1 it is recommended to be on the latest version available."},{"location":"spectator-ext/jvm-gc.html#dependencies","title":"Dependencies","pageTitle":"Garbage Collection","text":"com.netflix.spectator:spectator-ext-gc:0.62.0"},{"location":"spectator-ext/jvm-gc.html#start-reporting","title":"Start Reporting","pageTitle":"Garbage Collection","text":"Then in the initialization for the application: import com.netflix.spectator.gc.GcLogger;\n...\n// Keep a single instance of the logger\nGcLogger gc = new GcLogger();\ngc.start(null);"},{"location":"spectator-ext/jvm-gc.html#logging","title":"Logging","pageTitle":"Garbage Collection","text":"After GC events an INFO level log message will get reported using slf4j. This makes it easy to GC events in the context of other log messages for the application. The logger name is com.netflix.spectator.gc.GcLogger and the message will look like: ${GC_TYPE}: ${COLLECTOR_NAME}, id=${N}, at=${START_TIME}, duration=${T}ms, cause=[${CAUSE}], ${TOTAL_USAGE_BEFORE} => ${TOTAL_USAGE_AFTER} / ${MAX_SIZE} (${PERCENT_USAGE_BEFORE} => ${PERCENT_USAGE_AFTER}) The id can be used to verify events were not skipped or correlate with other sources like detailed GC logs. See GC causes for more details on the possible causes. Sample: 2014-08-31 02:02:24,724  INFO [com.netflix.spectator.gc.GcLogger] YOUNG: ParNew, id=5281, at=Sun Aug 31 02:02:24 UTC 2014, duration=2ms, cause=[Allocation Failure], 0.4G => 0.3G / 1.8G (24.3% => 16.6%)"},{"location":"spectator-ext/jvm-gc.html#metrics","title":"Metrics","pageTitle":"Garbage Collection","text":""},{"location":"spectator-ext/jvm-gc.html#jvm-gc-allocationrate","title":"jvm.gc.allocationRate","pageTitle":"Garbage Collection","text":"The allocation rate measures how fast the application is allocating memory. It is a counter that is incremented after a GC event by the amount youngGen.sizeBeforeGC. Technically, right now it is: youngGen.sizeBeforeGC - youngGen.sizeAfterGC However, youngGen.sizeAfterGC should be 0 and thus the size of young gen before the GC is the amount allocated since the previous GC event. Unit: bytes/second Dimensions: n/a"},{"location":"spectator-ext/jvm-gc.html#jvm-gc-promotionrate","title":"jvm.gc.promotionRate","pageTitle":"Garbage Collection","text":"The promotion rate measures how fast data is being moved from young generation into the old generation. It is a counter that is incremented after a GC event by the amount: abs(oldGen.sizeAfterGC - oldGen.sizeBeforeGC) Unit: bytes/second Dimensions: n/a"},{"location":"spectator-ext/jvm-gc.html#jvm-gc-livedatasize","title":"jvm.gc.liveDataSize","pageTitle":"Garbage Collection","text":"The live data size is the size of the old generation after a major GC. The image below shows how the live data size view compares to a metric showing the current size of the memory pool:  Unit: bytes Dimensions: n/a"},{"location":"spectator-ext/jvm-gc.html#jvm-gc-maxdatasize","title":"jvm.gc.maxDataSize","pageTitle":"Garbage Collection","text":"Maximum size for the old generation. Primary use-case is for gaining perspective on the the live data size. Unit: bytes Dimensions: n/a"},{"location":"spectator-ext/jvm-gc.html#jvm-gc-pause","title":"jvm.gc.pause","pageTitle":"Garbage Collection","text":"Timer reporting the pause time for a GC event. All of the values reported are stop the world pauses. Unit: statistic=max: seconds statistic=count: events/second statistic=totalTime: seconds/second Dimensions: action: action performed by the garbage collector ([javadoc](http://docs.oracle.com/javase/7/docs/jre/api/management/extension/com/sun/management/GarbageCollectionNotificationInfo.html#getGcAction())). There is no guarantee, but the typical values seen are end_of_major_GC and end_of_minor_GC. cause: cause that instigated GC ([javadoc](http://docs.oracle.com/javase/7/docs/jre/api/management/extension/com/sun/management/GarbageCollectionNotificationInfo.html#getGcCause())). For an explanation of common causes see the GC causes page."},{"location":"spectator-ext/jvm-gc.html#jvm-gc-concurrentphasetime","title":"jvm.gc.concurrentPhaseTime","pageTitle":"Garbage Collection","text":"Timer reporting time spent in concurrent phases of CMS. pauses. Unit: statistic=max: seconds statistic=count: events/second statistic=totalTime: seconds/second Dimensions: action: action performed by the garbage collector ([javadoc](http://docs.oracle.com/javase/7/docs/jre/api/management/extension/com/sun/management/GarbageCollectionNotificationInfo.html#getGcAction())). There is no guarantee, but the typical values seen are end_of_major_GC and end_of_minor_GC. cause: cause that instigated GC ([javadoc](http://docs.oracle.com/javase/7/docs/jre/api/management/extension/com/sun/management/GarbageCollectionNotificationInfo.html#getGcCause())). For an explanation of common causes see the GC causes page."},{"location":"spectator-ext/jvm-gc.html#alerting","title":"Alerting","pageTitle":"Garbage Collection","text":"This section assumes the data is available in Atlas, but users of other systems should be able to take the idea and make it work. For all of these alerts it is recommended to check them on instance. At Netflix that can be done by selecting the option in alert ui:"},{"location":"spectator-ext/jvm-gc.html#max-pause-time","title":"Max Pause Time","pageTitle":"Garbage Collection","text":"Example to trigger an alert if the pause time exceeds 500 milliseconds: name,jvm.gc.pause,:eq,\nstatistic,max,:eq,\n:and,\n:max,(,cause,),:by,\n0.5,:gt,\n$cause,:legend"},{"location":"spectator-ext/jvm-gc-causes.html#gc-causes","title":"GC Causes","pageTitle":"GC Causes","text":"The various GC causes aren’t well documented. The list provided here comes from the gcCause.cpp file in the jdk and we include some information on what these mean for the application."},{"location":"spectator-ext/jvm-gc-causes.html#system-gc-","title":"System.gc__","pageTitle":"GC Causes","text":"Something called [System.gc()](http://docs.oracle.com/javase/7/docs/api/java/lang/System.html#gc()). If you are seeing this once an hour it is likely related to the RMI GC interval. For more details see: Unexplained System.gc() calls due to Remote Method Invocation (RMI) or explict garbage collections sun.rmi.dgc.client.gcInterval"},{"location":"spectator-ext/jvm-gc-causes.html#fullgcalot","title":"FullGCAlot","pageTitle":"GC Causes","text":"Most likely you’ll never see this value. In debug builds of the jdk there is an option, -XX:+FullGCALot, that will trigger a full GC at a regular interval for testing purposes."},{"location":"spectator-ext/jvm-gc-causes.html#scavengealot","title":"ScavengeAlot","pageTitle":"GC Causes","text":"Most likely you’ll never see this value. In debug builds of the jdk there is an option, -XX:+ScavengeALot, that will trigger a minor GC at a regular interval for testing purposes."},{"location":"spectator-ext/jvm-gc-causes.html#allocation-profiler","title":"Allocation_Profiler","pageTitle":"GC Causes","text":"Prior to java 8 you would see this if running with the -Xaprof setting. It would be triggered just before the jvm exits. The -Xaprof option was removed in java 8."},{"location":"spectator-ext/jvm-gc-causes.html#jvmtienv-forcegarbagecollection","title":"JvmtiEnv_ForceGarbageCollection","pageTitle":"GC Causes","text":"Something called the JVM tool interface function ForceGarbageCollection. Look at the -agentlib param to java to see what agents are configured."},{"location":"spectator-ext/jvm-gc-causes.html#gclocker-initiated-gc","title":"GCLocker_Initiated_GC","pageTitle":"GC Causes","text":"The GC locker prevents GC from occurring when JNI code is in a critical region. If GC is needed while a thread is in a critical region, then it will allow them to complete, i.e. call the corresponding release function. Other threads will not be permitted to enter a critical region. Once all threads are out of critical regions a GC event will be triggered."},{"location":"spectator-ext/jvm-gc-causes.html#heap-inspection-initiated-gc","title":"Heap_Inspection_Initiated_GC","pageTitle":"GC Causes","text":"GC was initiated by an inspection operation on the heap. For example you can trigger this with jmap: $ jmap -histo:live <pid>"},{"location":"spectator-ext/jvm-gc-causes.html#heap-dump-initiated-gc","title":"Heap_Dump_Initiated_GC","pageTitle":"GC Causes","text":"GC was initiated before dumping the heap. For example you can trigger this with jmap: $ jmap -dump:live,format=b,file=heap.out <pid> Another common example would be clicking the Heap Dump button on the Monitor tab in jvisualvm."},{"location":"spectator-ext/jvm-gc-causes.html#whitebox-initiated-young-gc","title":"WhiteBox_Initiated_Young_GC","pageTitle":"GC Causes","text":"Most likely you’ll never see this value. Used for testing hotspot, it indicates something called sun.hotspot.WhiteBox.youngGC()."},{"location":"spectator-ext/jvm-gc-causes.html#no-gc","title":"No_GC","pageTitle":"GC Causes","text":"Used for CMS to indicate concurrent phases."},{"location":"spectator-ext/jvm-gc-causes.html#allocation-failure","title":"Allocation_Failure","pageTitle":"GC Causes","text":"Usually this means that there is an allocation request that is bigger than the available space in young generation and will typically be associated with a minor GC. For G1 this will likely be a major GC and it is more common to see G1_Evacuation_Pause for routine minor collections. On linux the jvm will trigger a GC if the kernel indicates there isn’t much memory left via mem_notify."},{"location":"spectator-ext/jvm-gc-causes.html#tenured-generation-full","title":"Tenured_Generation_Full","pageTitle":"GC Causes","text":"Not used?"},{"location":"spectator-ext/jvm-gc-causes.html#permanent-generation-full","title":"Permanent_Generation_Full","pageTitle":"GC Causes","text":"Triggered as a result of an allocation failure in PermGen. Pre java 8."},{"location":"spectator-ext/jvm-gc-causes.html#metadata-gc-threshold","title":"Metadata_GC_Threshold","pageTitle":"GC Causes","text":"Triggered as a result of an allocation failure in Metaspace. Metaspace replaced PermGen was added in java 8."},{"location":"spectator-ext/jvm-gc-causes.html#cms-generation-full","title":"CMS_Generation_Full","pageTitle":"GC Causes","text":"Not used?"},{"location":"spectator-ext/jvm-gc-causes.html#cms-initial-mark","title":"CMS_Initial_Mark","pageTitle":"GC Causes","text":"Initial mark phase of CMS, for more details see Phases of CMS. Unfortunately it doesn’t appear to be reported via the mbeans and we just get No_GC."},{"location":"spectator-ext/jvm-gc-causes.html#cms-final-remark","title":"CMS_Final_Remark","pageTitle":"GC Causes","text":"Remark phase of CMS, for more details see Phases of CMS. Unfortunately it doesn’t appear to be reported via the mbeans and we just get No_GC."},{"location":"spectator-ext/jvm-gc-causes.html#cms-concurrent-mark","title":"CMS_Concurrent_Mark","pageTitle":"GC Causes","text":"Concurrent mark phase of CMS, for more details see Phases of CMS. Unfortunately it doesn’t appear to be reported via the mbeans and we just get No_GC."},{"location":"spectator-ext/jvm-gc-causes.html#old-generation-expanded-on-last-scavenge","title":"Old_Generation_Expanded_On_Last_Scavenge","pageTitle":"GC Causes","text":"Not used?"},{"location":"spectator-ext/jvm-gc-causes.html#old-generation-too-full-to-scavenge","title":"Old_Generation_Too_Full_To_Scavenge","pageTitle":"GC Causes","text":"Not used?"},{"location":"spectator-ext/jvm-gc-causes.html#ergonomics","title":"Ergonomics","pageTitle":"GC Causes","text":"This indicates you are using the adaptive size policy, -XX:+UseAdaptiveSizePolicy and is on by default for recent versions, with the parallel collector (-XX:+UseParallelGC). For more details see The Why of GC Ergonomics."},{"location":"spectator-ext/jvm-gc-causes.html#g1-evacuation-pause","title":"G1_Evacuation_Pause","pageTitle":"GC Causes","text":"An evacuation pause is the most common young gen cause for G1 and indicates that it is copying live objects from one set of regions, young and sometimes young + old, to another set of regions. For more details see Understanding G1 GC Logs."},{"location":"spectator-ext/jvm-gc-causes.html#g1-humongous-allocation","title":"G1_Humongous_Allocation","pageTitle":"GC Causes","text":"A humongous allocation is one where the size is greater than 50% of the G1 region size. Before a humongous allocation the jvm checks if it should do a routine evacuation pause without regard to the actual allocation size, but if triggered due to this check the cause will be listed as humongous allocation. This cause is also used for any collections used to free up enough space for the allocation."},{"location":"spectator-ext/jvm-gc-causes.html#last-ditch-collection","title":"Last_ditch_collection","pageTitle":"GC Causes","text":"For perm gen (java 7 or earlier) and metaspace (java 8+) a last ditch collection will be triggered if an allocation fails and the memory pool cannot be expanded."},{"location":"spectator-ext/jvm-gc-causes.html#illegal-value-last-gc-cause-illegal-value","title":"ILLEGAL_VALUE_-_last_gc_cause_-_ILLEGAL_VALUE","pageTitle":"GC Causes","text":"Included for completeness, but you should never see this value."},{"location":"spectator-ext/jvm-memory-pools.html#memory-pools","title":"Memory Pools","pageTitle":"Memory Pools","text":"Uses the MemoryPoolMXBean provided by the JDK to monitor the sizes of java memory spaces such as perm gen, eden, old gen, etc."},{"location":"spectator-ext/jvm-memory-pools.html#getting-started","title":"Getting Started","pageTitle":"Memory Pools","text":"To get information about memory pools in spectator just setup registration of standard MXBeans. Note, if you are building an app at Netflix this should happen automatically via the normal platform initialization. import com.netflix.spectator.api.Spectator;\nimport com.netflix.spectator.jvm.Jmx;\n\nJmx.registerStandardMXBeans(Spectator.registry());"},{"location":"spectator-ext/jvm-memory-pools.html#metrics","title":"Metrics","pageTitle":"Memory Pools","text":""},{"location":"spectator-ext/jvm-memory-pools.html#jvm-memory-used","title":"jvm.memory.used","pageTitle":"Memory Pools","text":"Gauge reporting the current amount of memory used. For the young and old gen pools this metric will typically have a sawtooth pattern. For alerting or detecting memory pressure the live data size is probably a better option. Unit: bytes Dimensions: see metric dimensions"},{"location":"spectator-ext/jvm-memory-pools.html#jvm-memory-committed","title":"jvm.memory.committed","pageTitle":"Memory Pools","text":"Gauge reporting the current amount of memory committed. From the javadocs, committed is: The amount of memory (in bytes) that is guaranteed to be available for use by the Java virtual machine. The amount of committed memory may change over time (increase or decrease). The Java virtual machine may release memory to the system and committed could be less than init. committed will always be greater than or equal to used. Unit: bytes Dimensions: see metric dimensions"},{"location":"spectator-ext/jvm-memory-pools.html#jvm-memory-max","title":"jvm.memory.max","pageTitle":"Memory Pools","text":"Gauge reporting the max amount of memory that can be used. From the javadocs, max is: The maximum amount of memory (in bytes) that can be used for memory management. Its value may be undefined. The maximum amount of memory may change over time if defined. The amount of used and committed memory will always be less than or equal to max if max is defined. A memory allocation may fail if it attempts to increase the used memory such that used > committed even if used <= max would still be true (for example, when the system is low on virtual memory). Unit: bytes Dimensions: see metric dimensions"},{"location":"spectator-ext/thread-pools.html#thread-pools","title":"Thread Pools","pageTitle":"Thread Pools","text":"Java’s ThreadPoolExecutor exposes several properties that are useful to monitor to assess the health, performance, and configuration of the pool."},{"location":"spectator-ext/thread-pools.html#getting-started","title":"Getting Started","pageTitle":"Thread Pools","text":"To report thread pool metrics, one can attach a ThreadPoolMonitor in the following manner: import com.netflix.spectator.api.patterns.ThreadPoolMonitor;\n\nThreadPoolMonitor.attach(registry, myThreadPoolExecutor, \"my-thread-pool\"); The thread pool’s properties will be polled regularly in the background and will report metrics to the provided registry. The third parameter will be added to each metric as an id dimension, if provided. However, if the value is null or an empty string, then a default will be used as the id."},{"location":"spectator-ext/thread-pools.html#metrics","title":"Metrics","pageTitle":"Thread Pools","text":""},{"location":"spectator-ext/thread-pools.html#threadpool-taskcount","title":"threadpool.taskCount","pageTitle":"Thread Pools","text":"Counter of the total number of tasks that have been scheduled. Unit: tasks/second Data Source: ThreadPoolExecutor#getTaskCount()"},{"location":"spectator-ext/thread-pools.html#threadpool-completedtaskcount","title":"threadpool.completedTaskCount","pageTitle":"Thread Pools","text":"Counter of the total number of tasks that have completed. Unit: tasks/second Data Source: ThreadPoolExecutor#getCompletedTaskCount()"},{"location":"spectator-ext/thread-pools.html#threadpool-currentthreadsbusy","title":"threadpool.currentThreadsBusy","pageTitle":"Thread Pools","text":"Gauge showing the current number of threads actively doing work. Unit: count Data Source: ThreadPoolExecutor#getActiveCount()"},{"location":"spectator-ext/thread-pools.html#threadpool-maxthreads","title":"threadpool.maxThreads","pageTitle":"Thread Pools","text":"Gauge showing the current maximum number of threads configured for the pool. Unit: count Data Source: ThreadPoolExecutor#getMaximumPoolSize()"},{"location":"spectator-ext/thread-pools.html#threadpool-poolsize","title":"threadpool.poolSize","pageTitle":"Thread Pools","text":"Gauge showing the current size of the pool. Unit: count Data Source: ThreadPoolExecutor#getPoolSize()"},{"location":"spectator-ext/thread-pools.html#threadpool-corepoolsize","title":"threadpool.corePoolSize","pageTitle":"Thread Pools","text":"Gauge showing the current maximum number of core threads configured for the pool. Unit: count Data Source: ThreadPoolExecutor#getCorePoolSize()"},{"location":"spectator-ext/log4j1.html#log4j1-appender","title":"Log4j1 Appender","pageTitle":"Log4j1 Appender","text":"Custom appender for log4j1 to track the number of log messages reported. !!! note Log4j 1.x has reached end of life and is no longer supported by Apache. This extension is provided for some users that have difficulty moving to a supported version of log4j."},{"location":"spectator-ext/log4j1.html#getting-started","title":"Getting Started","pageTitle":"Log4j1 Appender","text":"To use it simply add a dependency: com.netflix.spectator:spectator-ext-log4j1:0.62.0 Then in your log4j configuration specify the com.netflix.spectator.log4j.SpectatorAppender. In a properties file it would look something like: log4j.rootLogger=ALL, A1\nlog4j.appender.A1=com.netflix.spectator.log4j.SpectatorAppender"},{"location":"spectator-ext/log4j1.html#metrics","title":"Metrics","pageTitle":"Log4j1 Appender","text":""},{"location":"spectator-ext/log4j1.html#log4j-nummessages","title":"log4j.numMessages","pageTitle":"Log4j1 Appender","text":"Counters showing the number of messages that have been passed to the appender. Unit: messages/second Dimensions: loglevel: standard log level of the events."},{"location":"spectator-ext/log4j2.html#log4j2-appender","title":"Log4j2 Appender","pageTitle":"Log4j2 Appender","text":"Custom appender for log4j2 to track the number of log messages reported."},{"location":"spectator-ext/log4j2.html#getting-started","title":"Getting Started","pageTitle":"Log4j2 Appender","text":"To use it simply add a dependency: com.netflix.spectator:spectator-ext-log4j2:0.62.0 Then in your application initialization: Registry registry = ...\nSpectatorAppender.addToRootLogger(\n    registry,             // Registry to use\n    \"spectator\",          // Name for the appender\n    false);               // Should stack traces be ignored? This will add the appender to the root logger and register a listener so it will get re-added if the configuration changes. You can also use the appender by specifying it in the log4j2 configuration, but this will cause some of the loggers in Spectator to get created before log4j is properly initialized and result in some lost log messages. With that caveat in mind, if you need the additional flexibility of using the configuration then specify the Spectator appender: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Configuration monitorInterval=\"5\" status=\"warn\">\n  <Appenders>\n    <Spectator name=\"root\"/>\n  </Appenders>\n  <Loggers>\n    <Root level=\"debug\">\n      <AppenderRef ref=\"root\"/>\n    </Root>\n  </Loggers>\n</Configuration>"},{"location":"spectator-ext/log4j2.html#metrics","title":"Metrics","pageTitle":"Log4j2 Appender","text":""},{"location":"spectator-ext/log4j2.html#log4j-nummessages","title":"log4j.numMessages","pageTitle":"Log4j2 Appender","text":"Counters showing the number of messages that have been passed to the appender. Unit: messages/second Dimensions: appender: name of the spectator appender. loglevel: standard log level of the events."},{"location":"spectator-ext/placeholders.html#placeholders","title":"Placeholders","pageTitle":"Placeholders","text":"The placeholders extension allows for identifiers to be created with dimensions that will get filled in based on the context when an activity occurs. The primary use-cases are to support: Optional dimensions that can be conditionally enabled. Pulling dimensions from another context such as a thread local store. This can make it is easier to share the across various parts of the code."},{"location":"spectator-ext/placeholders.html#dependencies","title":"Dependencies","pageTitle":"Placeholders","text":"To use the placeholders support add a dependency on: com.netflix.spectator:spectator-ext-placeholders:0.62.0"}]}